{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# hw6pr1iris:  NNETS!  including both clasification + regression for\n",
    "#    \n",
    "#              bitwise functions (MAJ + XOR)\n",
    "#              and our irises  (digits next week)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries!\n",
    "import numpy as np      # numpy is Python's \"array\" library\n",
    "import pandas as pd     # Pandas is Python's \"data\" library (\"dataframe\" == spreadsheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Start of MAJ (majority) example +++\n",
      "\n",
      "            input  -> pre   des  \n",
      "           [0 0 0] -> ?     0    \n",
      "           [0 0 1] -> ?     1    \n",
      "           [0 1 0] -> ?     1    \n",
      "           [0 1 1] -> ?     0    \n",
      "           [1 0 0] -> ?     1    \n",
      "           [1 0 1] -> ?     0    \n",
      "           [1 1 0] -> ?     0    \n",
      "           [1 1 1] -> ?     1    \n"
     ]
    }
   ],
   "source": [
    "print(\"+++ Start of MAJ (majority) example +++\\n\")\n",
    "\n",
    "# A = np.asarray( [ \n",
    "#                     [0,0,0,  0],  # three input bits, one output bit (MAJ)\n",
    "#                     [0,0,1,  0],   \n",
    "#                     [0,1,0,  0],  \n",
    "#                     [0,1,1,  1],   \n",
    "#                     [1,0,0,  0],  \n",
    "#                     [1,0,1,  1],   \n",
    "#                     [1,1,0,  1],  \n",
    "#                     [1,1,1,  1],\n",
    "#                 ])\n",
    "\n",
    "A = np.asarray( [ \n",
    "                    [0,0,0,  0],  # three input bits, one output bit (XOR) odd # of 1's\n",
    "                    [0,0,1,  1],   \n",
    "                    [0,1,0,  1],  \n",
    "                    [0,1,1,  0],   \n",
    "                    [1,0,0,  1],  \n",
    "                    [1,0,1,  0],   \n",
    "                    [1,1,0,  0],  \n",
    "                    [1,1,1,  1],\n",
    "                ])\n",
    "\n",
    "X_def = A[:,0:3].copy()   # data by definition in this case\n",
    "y_def = A[:,3].copy()\n",
    "\n",
    "def ascii_table(X,y):\n",
    "    \"\"\" print a table of binary inputs and outputs \"\"\"\n",
    "    print(f\"{'input ':>18s} -> {'pre':<5s} {'des':<5s}\") \n",
    "    for i in range(len(y)):\n",
    "        print(f\"{X[i,:]!s:>18s} -> {'?':<5s} {y[i]:<5.0f}\")   # !s is str ...\n",
    "        \n",
    "ascii_table(X_def,y_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            input  -> pre   des  \n",
      "           [0 0 0] -> ?     0    \n",
      "           [0 0 1] -> ?     1    \n",
      "           [0 1 0] -> ?     1    \n",
      "           [0 1 1] -> ?     0    \n",
      "           [1 0 0] -> ?     1    \n",
      "           [1 0 1] -> ?     0    \n",
      "           [1 1 0] -> ?     0    \n",
      "           [1 1 1] -> ?     1    \n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# we can scramble the remaining data if we want to...\n",
    "# \n",
    "SCRAMBLE = False\n",
    "if SCRAMBLE == True:\n",
    "    NUM_ROWS = len(y_def)\n",
    "    indices = np.random.permutation(NUM_ROWS)  # this scrambles the data each time\n",
    "    X_all = X_def[indices]\n",
    "    y_all = y_def[indices]\n",
    "else:\n",
    "    X_all = X_def  # don't scramble\n",
    "    y_all = y_def\n",
    "\n",
    "ascii_table(X_all,y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            input  -> pre   des  \n",
      "           [0 0 0] -> ?     0    \n",
      "           [0 0 1] -> ?     1    \n",
      "           [0 1 0] -> ?     1    \n",
      "           [0 1 1] -> ?     0    \n",
      "           [1 0 0] -> ?     1    \n",
      "           [1 0 1] -> ?     0    \n",
      "           [1 1 0] -> ?     0    \n",
      "           [1 1 1] -> ?     1    \n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# here, we are going to \"cheat\" by letting the full dataset \n",
    "# be both our training and testing sets.  \n",
    "#    We don't really need both pairs, but no worries.\n",
    "#\n",
    "X_train = X_all.copy()\n",
    "y_train = y_all.copy()\n",
    "\n",
    "X_test = X_all.copy()\n",
    "y_test = y_all.copy()\n",
    "\n",
    "ascii_table(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT using the input scaler:\n",
      "\n",
      "            input  -> pre   des  \n",
      "           [0 0 0] -> ?     0    \n",
      "           [0 0 1] -> ?     1    \n",
      "           [0 1 0] -> ?     1    \n",
      "           [0 1 1] -> ?     0    \n",
      "           [1 0 0] -> ?     1    \n",
      "           [1 0 1] -> ?     0    \n",
      "           [1 1 0] -> ?     0    \n",
      "           [1 1 1] -> ?     1    \n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# for many NNets, it's important to keep the feature values in the -1-to-1 range\n",
    "#    This is done through the \"StandardScaler\" in scikit-learn\n",
    "# \n",
    "USE_SCALER = False   # this variable is important! It tracks if we need to use the scaler...\n",
    "\n",
    "# we \"train the scaler\"  (computes the mean and standard deviation)\n",
    "if USE_SCALER == True:\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    # Scale with the training data! ave becomes 0; stdev becomes 1\n",
    "    scaler.fit(X_train)   \n",
    "    \n",
    "if USE_SCALER == True:    # if we're using the scaler, all inputs need to be scaled...\n",
    "    X_train_scaled = scaler.transform(X_train) # scale!\n",
    "    X_test_scaled = scaler.transform(X_test) # scale!\n",
    "    print(\"USING the input scaler:\\n\")\n",
    "else:\n",
    "    X_train_scaled = X_train  # not using the scaler\n",
    "    X_test_scaled = X_test  # not using the scaler\n",
    "    print(\"NOT using the input scaler:\\n\")\n",
    "    \n",
    "y_train_scaled = y_train  # the predicted/desired labels are not scaled\n",
    "y_test_scaled = y_test  # not using the scaler\n",
    "    \n",
    "ascii_table(X_train_scaled,y_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "++++++++++  TRAINING:  begin  +++++++++++++++\n",
      "\n",
      "\n",
      "Iteration 1, loss = 0.77287405\n",
      "Iteration 2, loss = 0.76179070\n",
      "Iteration 3, loss = 0.74908735\n",
      "Iteration 4, loss = 0.73709581\n",
      "Iteration 5, loss = 0.72732093\n",
      "Iteration 6, loss = 0.72033390\n",
      "Iteration 7, loss = 0.71593477\n",
      "Iteration 8, loss = 0.71345882\n",
      "Iteration 9, loss = 0.71209421\n",
      "Iteration 10, loss = 0.71112165\n",
      "Iteration 11, loss = 0.71004400\n",
      "Iteration 12, loss = 0.70861696\n",
      "Iteration 13, loss = 0.70681188\n",
      "Iteration 14, loss = 0.70474500\n",
      "Iteration 15, loss = 0.70260041\n",
      "Iteration 16, loss = 0.70056526\n",
      "Iteration 17, loss = 0.69878592\n",
      "Iteration 18, loss = 0.69734670\n",
      "Iteration 19, loss = 0.69626795\n",
      "Iteration 20, loss = 0.69551752\n",
      "Iteration 21, loss = 0.69502930\n",
      "Iteration 22, loss = 0.69472322\n",
      "Iteration 23, loss = 0.69452235\n",
      "Iteration 24, loss = 0.69436473\n",
      "Iteration 25, loss = 0.69420907\n",
      "Iteration 26, loss = 0.69403506\n",
      "Iteration 27, loss = 0.69383962\n",
      "Iteration 28, loss = 0.69363105\n",
      "Iteration 29, loss = 0.69342280\n",
      "Iteration 30, loss = 0.69322821\n",
      "Iteration 31, loss = 0.69305700\n",
      "Iteration 32, loss = 0.69291368\n",
      "Iteration 33, loss = 0.69279764\n",
      "Iteration 34, loss = 0.69270439\n",
      "Iteration 35, loss = 0.69262739\n",
      "Iteration 36, loss = 0.69255979\n",
      "Iteration 37, loss = 0.69249586\n",
      "Iteration 38, loss = 0.69243173\n",
      "Iteration 39, loss = 0.69236562\n",
      "Iteration 40, loss = 0.69229751\n",
      "Iteration 41, loss = 0.69222856\n",
      "Iteration 42, loss = 0.69216043\n",
      "Iteration 43, loss = 0.69209469\n",
      "Iteration 44, loss = 0.69203242\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 45, loss = 0.69197400\n",
      "Iteration 46, loss = 0.69192773\n",
      "Iteration 47, loss = 0.69188725\n",
      "Iteration 48, loss = 0.69185153\n",
      "Iteration 49, loss = 0.69181970\n",
      "Iteration 50, loss = 0.69179102\n",
      "Iteration 51, loss = 0.69176486\n",
      "Iteration 52, loss = 0.69174072\n",
      "Iteration 53, loss = 0.69171817\n",
      "Iteration 54, loss = 0.69169688\n",
      "Iteration 55, loss = 0.69167659\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 56, loss = 0.69165709\n",
      "Iteration 57, loss = 0.69164037\n",
      "Iteration 58, loss = 0.69162514\n",
      "Iteration 59, loss = 0.69161122\n",
      "Iteration 60, loss = 0.69159845\n",
      "Iteration 61, loss = 0.69158671\n",
      "Iteration 62, loss = 0.69157588\n",
      "Iteration 63, loss = 0.69156586\n",
      "Iteration 64, loss = 0.69155655\n",
      "Iteration 65, loss = 0.69154789\n",
      "Iteration 66, loss = 0.69153980\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 67, loss = 0.69153222\n",
      "Iteration 68, loss = 0.69152557\n",
      "Iteration 69, loss = 0.69151953\n",
      "Iteration 70, loss = 0.69151403\n",
      "Iteration 71, loss = 0.69150903\n",
      "Iteration 72, loss = 0.69150447\n",
      "Iteration 73, loss = 0.69150031\n",
      "Iteration 74, loss = 0.69149651\n",
      "Iteration 75, loss = 0.69149303\n",
      "Iteration 76, loss = 0.69148984\n",
      "Iteration 77, loss = 0.69148691\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 78, loss = 0.69148421\n",
      "Iteration 79, loss = 0.69148181\n",
      "Iteration 80, loss = 0.69147964\n",
      "Iteration 81, loss = 0.69147768\n",
      "Iteration 82, loss = 0.69147590\n",
      "Iteration 83, loss = 0.69147429\n",
      "Iteration 84, loss = 0.69147283\n",
      "Iteration 85, loss = 0.69147150\n",
      "Iteration 86, loss = 0.69147029\n",
      "Iteration 87, loss = 0.69146920\n",
      "Iteration 88, loss = 0.69146820\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 89, loss = 0.69146728\n",
      "Iteration 90, loss = 0.69146647\n",
      "Iteration 91, loss = 0.69146573\n",
      "Iteration 92, loss = 0.69146507\n",
      "Iteration 93, loss = 0.69146447\n",
      "Iteration 94, loss = 0.69146393\n",
      "Iteration 95, loss = 0.69146344\n",
      "Iteration 96, loss = 0.69146299\n",
      "Iteration 97, loss = 0.69146259\n",
      "Iteration 98, loss = 0.69146223\n",
      "Iteration 99, loss = 0.69146190\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 100, loss = 0.69146160\n",
      "Iteration 101, loss = 0.69146133\n",
      "Iteration 102, loss = 0.69146109\n",
      "Iteration 103, loss = 0.69146087\n",
      "Iteration 104, loss = 0.69146068\n",
      "Iteration 105, loss = 0.69146050\n",
      "Iteration 106, loss = 0.69146034\n",
      "Iteration 107, loss = 0.69146019\n",
      "Iteration 108, loss = 0.69146006\n",
      "Iteration 109, loss = 0.69145994\n",
      "Iteration 110, loss = 0.69145984\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 111, loss = 0.69145974\n",
      "Iteration 112, loss = 0.69145965\n",
      "Iteration 113, loss = 0.69145958\n",
      "Iteration 114, loss = 0.69145951\n",
      "Iteration 115, loss = 0.69145944\n",
      "Iteration 116, loss = 0.69145939\n",
      "Iteration 117, loss = 0.69145933\n",
      "Iteration 118, loss = 0.69145929\n",
      "Iteration 119, loss = 0.69145925\n",
      "Iteration 120, loss = 0.69145921\n",
      "Iteration 121, loss = 0.69145917\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 122, loss = 0.69145914\n",
      "Iteration 123, loss = 0.69145911\n",
      "Iteration 124, loss = 0.69145909\n",
      "Iteration 125, loss = 0.69145907\n",
      "Iteration 126, loss = 0.69145905\n",
      "Iteration 127, loss = 0.69145903\n",
      "Iteration 128, loss = 0.69145901\n",
      "Iteration 129, loss = 0.69145900\n",
      "Iteration 130, loss = 0.69145898\n",
      "Iteration 131, loss = 0.69145897\n",
      "Iteration 132, loss = 0.69145896\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "\n",
      "\n",
      "++++++++++  TRAINING:   end  +++++++++++++++\n",
      "\n",
      "\n",
      "The analog prediction error (the loss) is 0.6914589610963539\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# import our NNet library (within scikit-learn)\n",
    "#       import error?  run a cell with !conda update scikit-learn\n",
    "#\n",
    "from sklearn.neural_network import MLPClassifier   #multilayer perceptron\n",
    "\n",
    "#\n",
    "# Here's where you can change the number of layers, neurons, and other parameters:\n",
    "#\n",
    "nn_classifier = MLPClassifier(hidden_layer_sizes=(4,), max_iter=200, activation=\"tanh\",\n",
    "                    solver='sgd', verbose=True, shuffle=True,\n",
    "                    random_state=None, # reproduceability!\n",
    "                    learning_rate_init=.1, learning_rate = 'adaptive')\n",
    "\n",
    "# documentation:\n",
    "# scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html \n",
    "#     Try verbose / activation \"relu\" / other network sizes ...\n",
    "\n",
    "print(\"\\n\\n++++++++++  TRAINING:  begin  +++++++++++++++\\n\\n\")\n",
    "nn_classifier.fit(X_train_scaled, y_train_scaled)\n",
    "print(\"\\n\\n++++++++++  TRAINING:   end  +++++++++++++++\\n\\n\")\n",
    "\n",
    "print(f\"The analog prediction error (the loss) is {nn_classifier.loss_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            input  ->  pred   desr \n",
      "           [0 0 0] ->   1      0      incorrect: [0.48554165 0.51445835]\n",
      "           [0 0 1] ->   1      1              \n",
      "           [0 1 0] ->   0      1      incorrect: [0.52278683 0.47721317]\n",
      "           [0 1 1] ->   0      0              \n",
      "           [1 0 0] ->   1      1              \n",
      "           [1 0 1] ->   0      0              \n",
      "           [1 1 0] ->   0      0              \n",
      "           [1 1 1] ->   0      1      incorrect: [0.52873967 0.47126033]\n",
      "\n",
      "correct predictions: 5 out of 8\n",
      "\n",
      "\n",
      "+++++ parameters, weights, etc. +++++\n",
      "\n",
      "\n",
      "weights/coefficients:\n",
      "\n",
      "[[-0.91722979  0.09826731 -0.14541562  0.18204201]\n",
      " [-0.34923858  0.1812823   0.08239441 -0.34065874]\n",
      " [ 0.6524091   0.84851153 -0.6849058  -0.77400868]]\n",
      "[[ 0.28503503]\n",
      " [ 0.02972361]\n",
      " [-0.10951727]\n",
      " [ 0.50178926]]\n",
      "\n",
      "intercepts: [array([-0.86837788, -0.58654872, -0.28976761, -0.55468155]), array([0.49524402])]\n",
      "\n",
      "all parameters: {'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (4,), 'learning_rate': 'adaptive', 'learning_rate_init': 0.1, 'max_fun': 15000, 'max_iter': 200, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': None, 'shuffle': True, 'solver': 'sgd', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': True, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# what can we see about our nnet?\n",
    "#\n",
    "#\n",
    "# how did it do on the training data?   (It's the same as the testing data, in this case!)\n",
    "#\n",
    "\n",
    "#\n",
    "# which one do we want: classifier or regressor?\n",
    "#\n",
    "\n",
    "def ascii_table_for_classifier(X,y,nn):\n",
    "    \"\"\" a table including predictions using nn.predict \"\"\"\n",
    "    predictions = nn.predict(X) # all predictions\n",
    "    prediction_probs = nn.predict_proba(X) # all prediction probabilities\n",
    "    # count correct\n",
    "#     print(predictions)\n",
    "#     print(prediction_probs)\n",
    "    num_correct = 0\n",
    "    # printing\n",
    "    \n",
    "    print(f\"{'input ':>18s} -> {'pred':^6s} {'desr':^6s}\") \n",
    "    for i in range(len(y)):\n",
    "        pred = predictions[i]\n",
    "        pred_probs = prediction_probs[i,:]\n",
    "        desired = y[i]\n",
    "        if pred != desired: result = \"  incorrect: \" + str(pred_probs)\n",
    "        else: result = \"\"; num_correct += 1\n",
    "        # could use X_train below to show the unscaled, if sought...\n",
    "        print(f\"{X[i,:]!s:>18s} -> {pred:^6.0f} {desired:^6.0f} {result:^10s}\") \n",
    "    print(f\"\\ncorrect predictions: {num_correct} out of {len(y)}\")\n",
    "    \n",
    "\n",
    "\n",
    "#\n",
    "# let's see how it did on the test data (also the training data!)\n",
    "#\n",
    "ascii_table_for_classifier(X_test_scaled,\n",
    "                           y_test_scaled,\n",
    "                           nn_classifier)   # this is our own f'n, above\n",
    "#\n",
    "# other things...\n",
    "#\n",
    "nn = nn_classifier  # less to type?\n",
    "print(\"\\n\\n+++++ parameters, weights, etc. +++++\\n\")\n",
    "print(f\"\\nweights/coefficients:\\n\")\n",
    "for wts in nn.coefs_:\n",
    "    print(wts)\n",
    "print(f\"\\nintercepts: {nn.intercepts_}\")\n",
    "print(f\"\\nall parameters: {nn.get_params()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input features are [[0.2  0.65 0.7 ]]\n",
      "nn.predict_proba ==  [[0.5134946 0.4865054]]\n",
      "prediction: [0]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# we have a predictive model!  Let's try it out...\n",
    "#\n",
    "\n",
    "def make_prediction( Features, nn ):\n",
    "    \"\"\" be sure that Features has the right shape! \"\"\"\n",
    "    row = np.array( [Features] )  # makes an array-row\n",
    "    if USE_SCALER == True:\n",
    "        row = scaler.transform(row)\n",
    "    print(\"input features are\", row)\n",
    "    print(\"nn.predict_proba == \", nn.predict_proba(row))\n",
    "    prediction = nn.predict(row)\n",
    "    return prediction\n",
    "    \n",
    "# our features\n",
    "Features = [ 0.2, 0.65, 0.7 ]\n",
    "prediction = make_prediction(Features, nn_classifier)\n",
    "print(f\"prediction: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# NNets are more natural regressors than classifiers...\n",
    "#\n",
    "#   That is, they naturally output continuous, floating-point values\n",
    "#   instead of a category or choice-among-labels.\n",
    "#\n",
    "#   So, let's try to predict our binary function as a floating point output instead...\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "++++++++++  TRAINING:  begin  +++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "++++++++++  TRAINING:   end  +++++++++++++++\n",
      "\n",
      "\n",
      "The (squared) prediction error (the loss) is 0.00020034328475695065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chiachinghsieh/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# MLPRegressor predicts floating-point outputs\n",
    "#\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "nn_regressor = MLPRegressor(hidden_layer_sizes=(4,), max_iter=200, activation=\"tanh\",\n",
    "                    solver='sgd', verbose=False, shuffle=True,\n",
    "                    random_state=None, # reproduceability!\n",
    "                    learning_rate_init=.1, learning_rate = 'adaptive')\n",
    "\n",
    "print(\"\\n\\n++++++++++  TRAINING:  begin  +++++++++++++++\\n\\n\")\n",
    "nn_regressor.fit(X_train, y_train)\n",
    "print(\"\\n\\n++++++++++  TRAINING:   end  +++++++++++++++\\n\\n\")\n",
    "\n",
    "print(f\"The (squared) prediction error (the loss) is {nn_regressor.loss_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            input  ->   pred    desr    absdiff  \n",
      "           [0 0 0] ->  -0.001  +0.000    0.001   \n",
      "           [0 0 1] ->  +1.000  +1.000    0.000   \n",
      "           [0 1 0] ->  +1.018  +1.000    0.018   \n",
      "           [0 1 1] ->  -0.008  +0.000    0.008   \n",
      "           [1 0 0] ->  +1.006  +1.000    0.006   \n",
      "           [1 0 1] ->  -0.009  +0.000    0.009   \n",
      "           [1 1 0] ->  -0.015  +0.000    0.015   \n",
      "           [1 1 1] ->  +1.012  +1.000    0.012   \n",
      "\n",
      "average abs error: 0.008829880592530842\n",
      "\n",
      "\n",
      "+++++ parameters, weights, etc. +++++\n",
      "\n",
      "\n",
      "weights/coefficients:\n",
      "\n",
      "[[-1.75404937 -0.75348188  0.17248849 -0.78518314]\n",
      " [ 2.02597199  0.89119138  0.01290878  0.97679192]\n",
      " [-1.8313514  -0.68939382  0.65523024 -0.71254168]]\n",
      "[[-1.7386689 ]\n",
      " [ 1.60021224]\n",
      " [-0.5215019 ]\n",
      " [ 1.49590365]]\n",
      "\n",
      "intercepts: [array([ 0.75617772,  1.03275051,  0.17818079, -0.5712695 ]), array([0.73343015])]\n",
      "\n",
      "all parameters: {'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (4,), 'learning_rate': 'adaptive', 'learning_rate_init': 0.1, 'max_fun': 15000, 'max_iter': 200, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': None, 'shuffle': True, 'solver': 'sgd', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# how did it do? now we're making progress (by regressing)\n",
    "#\n",
    "\n",
    "def ascii_table_for_regressor(X,y,nn):\n",
    "    \"\"\" a table including predictions using nn.predict \"\"\"\n",
    "    predictions = nn.predict(X) # all predictions\n",
    "    # measure error\n",
    "    error = 0.0\n",
    "    # printing\n",
    "    print(f\"{'input ':>18s} ->  {'pred':^6s}  {'desr':^6s}  {'absdiff':^10s}\") \n",
    "    for i in range(len(y)):\n",
    "        pred = predictions[i]\n",
    "        desired = y[i]\n",
    "        result = abs(desired - pred)\n",
    "        error += result\n",
    "        print(f\"{X[i,:]!s:>18s} ->  {pred:<+6.3f}  {desired:<+6.3f}  {result:^10.3f}\") \n",
    "    print(f\"\\naverage abs error: {error/len(y)}\")\n",
    "    \n",
    "\n",
    "\n",
    "#\n",
    "# let's see how it did on the test data (also the training data!)\n",
    "#\n",
    "ascii_table_for_regressor(X_test_scaled,\n",
    "                          y_test_scaled,\n",
    "                          nn_regressor)   # this is our own f'n, above\n",
    "#\n",
    "# other things...\n",
    "#\n",
    "nn = nn_regressor  # less to type?\n",
    "print(\"\\n\\n+++++ parameters, weights, etc. +++++\\n\")\n",
    "print(f\"\\nweights/coefficients:\\n\")\n",
    "for wts in nn.coefs_:\n",
    "    print(wts)\n",
    "print(f\"\\nintercepts: {nn.intercepts_}\")\n",
    "print(f\"\\nall parameters: {nn.get_params()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input features are [[1. 1. 1.]]\n",
      "nn.predict(row) ==  [1.0120869]\n",
      "prediction: [1.0120869]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# again, we have a predictive model, now a regressor.  Let's try it out...\n",
    "#\n",
    "\n",
    "def make_prediction( Features, nn ):\n",
    "    \"\"\" be sure that Features has the right shape! \"\"\"\n",
    "    row = np.array( [Features] )  # makes an array-row\n",
    "    if USE_SCALER == True:\n",
    "        row = scaler.transform(row)\n",
    "    print(\"input features are\", row)\n",
    "    prediction = nn.predict(row)\n",
    "    print(\"nn.predict(row) == \", prediction)\n",
    "    return prediction\n",
    "    \n",
    "# our features\n",
    "Features = [ 1.0, 1.0, 1.0 ]\n",
    "prediction = make_prediction(Features, nn_regressor)\n",
    "print(f\"prediction: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Onward to the iris data!\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris.csv : file read into a pandas dataframe.\n"
     ]
    }
   ],
   "source": [
    "# let's read in our flower data...\n",
    "# \n",
    "# for read_csv, use header=0 when row 0 is a header row\n",
    "# \n",
    "filename = 'iris.csv'\n",
    "df = pd.read_csv(filename, header=0)   # encoding=\"latin1\" et al.\n",
    "print(f\"{filename} : file read into a pandas dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepallen</th>\n",
       "      <th>sepalwid</th>\n",
       "      <th>petallen</th>\n",
       "      <th>petalwid</th>\n",
       "      <th>irisname</th>\n",
       "      <th>adapted from https://en.wikipedia.org/wiki/Iris_flower_data_set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000e-01</td>\n",
       "      <td>setosa</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>setosa</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.000000e-01</td>\n",
       "      <td>setosa</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.000000e-01</td>\n",
       "      <td>setosa</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>2.000000e-01</td>\n",
       "      <td>setosa</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.200000e+00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2.300000e+00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>4.2</td>\n",
       "      <td>42.0</td>\n",
       "      <td>4242.0</td>\n",
       "      <td>4.200000e+42</td>\n",
       "      <td>alieniris</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>4.2</td>\n",
       "      <td>42.0</td>\n",
       "      <td>4242.0</td>\n",
       "      <td>4.200000e+42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepallen  sepalwid  petallen      petalwid   irisname  \\\n",
       "0         4.6       3.6       1.0  2.000000e-01     setosa   \n",
       "1         4.3       3.0       1.1  1.000000e-01     setosa   \n",
       "2         5.0       3.2       1.2  2.000000e-01     setosa   \n",
       "3         5.8       4.0       1.2  2.000000e-01     setosa   \n",
       "4         4.4       3.0       1.3  2.000000e-01     setosa   \n",
       "..        ...       ...       ...           ...        ...   \n",
       "138       7.7       3.8       6.7  2.200000e+00  virginica   \n",
       "139       7.7       2.8       6.7  2.000000e+00  virginica   \n",
       "140       7.7       2.6       6.9  2.300000e+00  virginica   \n",
       "141       4.2      42.0    4242.0  4.200000e+42  alieniris   \n",
       "142       4.2      42.0    4242.0  4.200000e+42        NaN   \n",
       "\n",
       "     adapted from https://en.wikipedia.org/wiki/Iris_flower_data_set  \n",
       "0                                                  NaN                \n",
       "1                                                  NaN                \n",
       "2                                                  NaN                \n",
       "3                                                  NaN                \n",
       "4                                                  NaN                \n",
       "..                                                 ...                \n",
       "138                                                NaN                \n",
       "139                                                NaN                \n",
       "140                                                NaN                \n",
       "141                                                NaN                \n",
       "142                                                NaN                \n",
       "\n",
       "[143 rows x 6 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# a dataframe is a \"spreadsheet in Python\"   (seems to have an extra column!)\n",
    "#\n",
    "pd.set_option('display.max_rows', 10)     # None for no limit; default: 10\n",
    "# pd.set_option('display.min_rows', 150)   # min_rows is not universally supported...\n",
    "# let's view it!\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 143 entries, 0 to 142\n",
      "Data columns (total 6 columns):\n",
      " #   Column                                                           Non-Null Count  Dtype  \n",
      "---  ------                                                           --------------  -----  \n",
      " 0   sepallen                                                         143 non-null    float64\n",
      " 1   sepalwid                                                         143 non-null    float64\n",
      " 2   petallen                                                         143 non-null    float64\n",
      " 3   petalwid                                                         143 non-null    float64\n",
      " 4   irisname                                                         142 non-null    object \n",
      " 5   adapted from https://en.wikipedia.org/wiki/Iris_flower_data_set  0 non-null      float64\n",
      "dtypes: float64(5), object(1)\n",
      "memory usage: 6.8+ KB\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# let's look at our pandas dataframe   (Aargh: that extra column!)\n",
    "#\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 143 entries, 0 to 142\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   sepallen  143 non-null    float64\n",
      " 1   sepalwid  143 non-null    float64\n",
      " 2   petallen  143 non-null    float64\n",
      " 3   petalwid  143 non-null    float64\n",
      " 4   irisname  142 non-null    object \n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 5.7+ KB\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# let's drop that last column (dropping is usually by _name_):\n",
    "#\n",
    "#   if you want a list of the column names use df.columns\n",
    "col5name = df.columns[5]  # get column name at index 5\n",
    "df_clean = df.drop(columns=[col5name])  # drop by name is typical\n",
    "df_clean.info()                         # should be happier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLUMNS is Index(['sepallen', 'sepalwid', 'petallen', 'petalwid', 'irisname'], dtype='object')\n",
      "\n",
      "COLUMNS[0] is sepallen\n",
      "\n",
      "COL_INDEX is {'sepallen': 0, 'sepalwid': 1, 'petallen': 2, 'petalwid': 3, 'irisname': 4}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# let's keep our column names in variables, for reference\n",
    "#\n",
    "COLUMNS = df_clean.columns            # \"list\" of columns\n",
    "print(f\"COLUMNS is {COLUMNS}\\n\")  \n",
    "  # It's a \"pandas\" list, called an Index\n",
    "  # use it just as a Python list of strings:\n",
    "print(f\"COLUMNS[0] is {COLUMNS[0]}\\n\")\n",
    "\n",
    "# let's create a dictionary to look up any column index by name\n",
    "COL_INDEX = {}\n",
    "for i, name in enumerate(COLUMNS):\n",
    "    COL_INDEX[name] = i  # using the name (as key), look up the value (i)\n",
    "print(f\"COL_INDEX is {COL_INDEX}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 143 entries, 0 to 142\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   sepallen  143 non-null    float64\n",
      " 1   sepalwid  143 non-null    float64\n",
      " 2   petallen  143 non-null    float64\n",
      " 3   petalwid  143 non-null    float64\n",
      " 4   irisname  142 non-null    object \n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 5.7+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepallen</th>\n",
       "      <th>sepalwid</th>\n",
       "      <th>petallen</th>\n",
       "      <th>petalwid</th>\n",
       "      <th>irisname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000e-01</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.000000e-01</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.000000e-01</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>2.000000e-01</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.200000e+00</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2.300000e+00</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>4.2</td>\n",
       "      <td>42.0</td>\n",
       "      <td>4242.0</td>\n",
       "      <td>4.200000e+42</td>\n",
       "      <td>alieniris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>4.2</td>\n",
       "      <td>42.0</td>\n",
       "      <td>4242.0</td>\n",
       "      <td>4.200000e+42</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepallen  sepalwid  petallen      petalwid   irisname\n",
       "0         4.6       3.6       1.0  2.000000e-01     setosa\n",
       "1         4.3       3.0       1.1  1.000000e-01     setosa\n",
       "2         5.0       3.2       1.2  2.000000e-01     setosa\n",
       "3         5.8       4.0       1.2  2.000000e-01     setosa\n",
       "4         4.4       3.0       1.3  2.000000e-01     setosa\n",
       "..        ...       ...       ...           ...        ...\n",
       "138       7.7       3.8       6.7  2.200000e+00  virginica\n",
       "139       7.7       2.8       6.7  2.000000e+00  virginica\n",
       "140       7.7       2.6       6.9  2.300000e+00  virginica\n",
       "141       4.2      42.0    4242.0  4.200000e+42  alieniris\n",
       "142       4.2      42.0    4242.0  4.200000e+42        NaN\n",
       "\n",
       "[143 rows x 5 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# let's look at our cleaned-up dataframe...\n",
    "#\n",
    "df_clean.info()   \n",
    "#\n",
    "# notice that the non-null is _different_ for irisname!\n",
    "df_clean   # show a table! (the problem rows are the last two...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 142 entries, 0 to 141\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   sepallen  142 non-null    float64\n",
      " 1   sepalwid  142 non-null    float64\n",
      " 2   petallen  142 non-null    float64\n",
      " 3   petalwid  142 non-null    float64\n",
      " 4   irisname  142 non-null    object \n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 6.7+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepallen</th>\n",
       "      <th>sepalwid</th>\n",
       "      <th>petallen</th>\n",
       "      <th>petalwid</th>\n",
       "      <th>irisname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000e-01</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.000000e-01</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.000000e-01</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>2.000000e-01</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>7.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.100000e+00</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.200000e+00</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2.300000e+00</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>4.2</td>\n",
       "      <td>42.0</td>\n",
       "      <td>4242.0</td>\n",
       "      <td>4.200000e+42</td>\n",
       "      <td>alieniris</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepallen  sepalwid  petallen      petalwid   irisname\n",
       "0         4.6       3.6       1.0  2.000000e-01     setosa\n",
       "1         4.3       3.0       1.1  1.000000e-01     setosa\n",
       "2         5.0       3.2       1.2  2.000000e-01     setosa\n",
       "3         5.8       4.0       1.2  2.000000e-01     setosa\n",
       "4         4.4       3.0       1.3  2.000000e-01     setosa\n",
       "..        ...       ...       ...           ...        ...\n",
       "137       7.6       3.0       6.6  2.100000e+00  virginica\n",
       "138       7.7       3.8       6.7  2.200000e+00  virginica\n",
       "139       7.7       2.8       6.7  2.000000e+00  virginica\n",
       "140       7.7       2.6       6.9  2.300000e+00  virginica\n",
       "141       4.2      42.0    4242.0  4.200000e+42  alieniris\n",
       "\n",
       "[142 rows x 5 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# typically, after dropping columns we don't want, \n",
    "#   we drop rows with missing data (other approaches are possible, too)\n",
    "#\n",
    "df_full = df_clean.dropna()   # this removes all rows with missing data (\"na\")\n",
    "df_full.info()                # it's \"full\" because it has no missing data\n",
    "df_full\n",
    "#\n",
    "# notice that _all_ of the rows now have 142 non-null items\n",
    "#    also, the last row isn't real data... we'll handle it next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(141, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepallen</th>\n",
       "      <th>sepalwid</th>\n",
       "      <th>petallen</th>\n",
       "      <th>petalwid</th>\n",
       "      <th>irisname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>7.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.2</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepallen  sepalwid  petallen  petalwid   irisname\n",
       "0         4.6       3.6       1.0       0.2     setosa\n",
       "1         4.3       3.0       1.1       0.1     setosa\n",
       "2         5.0       3.2       1.2       0.2     setosa\n",
       "3         5.8       4.0       1.2       0.2     setosa\n",
       "4         4.4       3.0       1.3       0.2     setosa\n",
       "..        ...       ...       ...       ...        ...\n",
       "136       7.9       3.8       6.4       2.0  virginica\n",
       "137       7.6       3.0       6.6       2.1  virginica\n",
       "138       7.7       3.8       6.7       2.2  virginica\n",
       "139       7.7       2.8       6.7       2.0  virginica\n",
       "140       7.7       2.6       6.9       2.3  virginica\n",
       "\n",
       "[141 rows x 5 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "# get rid of last row!\n",
    "#\n",
    "df_final = df_full.iloc[0:-1]     # not the syntax I would choose\n",
    "print(df_final.shape)\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setosa maps to 0\n",
      "versicolor maps to 1\n",
      "virginica maps to 2\n"
     ]
    }
   ],
   "source": [
    "# all of scikit-learn's ML routines need numbers, not strings\n",
    "#   ... even for categories/classifications (like species!)\n",
    "#   so, we will convert the flower-species to numbers:\n",
    "\n",
    "SPECIES = ['setosa','versicolor','virginica']   # int to str\n",
    "SPECIES_INDEX = {'setosa':0,'versicolor':1,'virginica':2}  # str to int\n",
    "\n",
    "def convert_species(speciesname):\n",
    "    \"\"\" return the species index (a unique integer/category) \"\"\"\n",
    "    #print(f\"converting {speciesname}...\")\n",
    "    return SPECIES_INDEX[speciesname]\n",
    "\n",
    "# Let's try it out...\n",
    "for name in SPECIES:\n",
    "    print(f\"{name} maps to {convert_species(name)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chiachinghsieh/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# we can \"apply\" a function to a whole column\n",
    "#   it may give a warning; here, this is ok ...\n",
    "#\n",
    "\n",
    "df_final['irisname'] = df_final['irisname'].apply(convert_species)\n",
    "\n",
    "# Don't run this twice: the data will be different the second time!\n",
    "#   (In reality, feel free to go back and re-run cells to re-establish things... :-)\n",
    "#    Don't worry about the (possible)  \"SettingWithCopyWarning\" here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepallen</th>\n",
       "      <th>sepalwid</th>\n",
       "      <th>petallen</th>\n",
       "      <th>petalwid</th>\n",
       "      <th>irisname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>7.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepallen  sepalwid  petallen  petalwid  irisname\n",
       "0         4.6       3.6       1.0       0.2         0\n",
       "1         4.3       3.0       1.1       0.1         0\n",
       "2         5.0       3.2       1.2       0.2         0\n",
       "3         5.8       4.0       1.2       0.2         0\n",
       "4         4.4       3.0       1.3       0.2         0\n",
       "..        ...       ...       ...       ...       ...\n",
       "136       7.9       3.8       6.4       2.0         2\n",
       "137       7.6       3.0       6.6       2.1         2\n",
       "138       7.7       3.8       6.7       2.2         2\n",
       "139       7.7       2.8       6.7       2.0         2\n",
       "140       7.7       2.6       6.9       2.3         2\n",
       "\n",
       "[141 rows x 5 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# let's see it!  (this is safe to run many times...)\n",
    "#\n",
    "df_final         # print(df_final.to_string())  # for _all_ rows..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.6 3.6 1.  0.2 0. ]\n",
      " [4.3 3.  1.1 0.1 0. ]\n",
      " [5.  3.2 1.2 0.2 0. ]\n",
      " [5.8 4.  1.2 0.2 0. ]\n",
      " [4.4 3.  1.3 0.2 0. ]\n",
      " [4.4 3.2 1.3 0.2 0. ]\n",
      " [4.5 2.3 1.3 0.3 0. ]\n",
      " [4.7 3.2 1.3 0.2 0. ]\n",
      " [5.  3.5 1.3 0.3 0. ]\n",
      " [5.4 3.9 1.3 0.4 0. ]\n",
      " [5.5 3.5 1.3 0.2 0. ]\n",
      " [4.4 2.9 1.4 0.2 0. ]\n",
      " [4.6 3.4 1.4 0.3 0. ]\n",
      " [4.6 3.2 1.4 0.2 0. ]\n",
      " [4.8 3.  1.4 0.1 0. ]\n",
      " [4.8 3.  1.4 0.3 0. ]\n",
      " [4.9 3.  1.4 0.2 0. ]\n",
      " [5.  3.6 1.4 0.2 0. ]\n",
      " [5.  3.3 1.4 0.2 0. ]\n",
      " [5.1 3.5 1.4 0.2 0. ]\n",
      " [5.1 3.5 1.4 0.3 0. ]\n",
      " [5.2 3.4 1.4 0.2 0. ]\n",
      " [5.5 4.2 1.4 0.2 0. ]\n",
      " [4.6 3.1 1.5 0.2 0. ]\n",
      " [4.9 3.1 1.5 0.1 0. ]\n",
      " [4.9 3.1 1.5 0.1 0. ]\n",
      " [4.9 3.1 1.5 0.1 0. ]\n",
      " [5.  3.4 1.5 0.2 0. ]\n",
      " [5.1 3.8 1.5 0.3 0. ]\n",
      " [5.1 3.7 1.5 0.4 0. ]\n",
      " [5.1 3.4 1.5 0.2 0. ]\n",
      " [5.2 3.5 1.5 0.2 0. ]\n",
      " [5.3 3.7 1.5 0.2 0. ]\n",
      " [5.4 3.7 1.5 0.2 0. ]\n",
      " [5.7 4.4 1.5 0.4 0. ]\n",
      " [4.7 3.2 1.6 0.2 0. ]\n",
      " [4.8 3.4 1.6 0.2 0. ]\n",
      " [5.  3.  1.6 0.2 0. ]\n",
      " [5.  3.4 1.6 0.4 0. ]\n",
      " [5.  3.5 1.6 0.6 0. ]\n",
      " [5.1 3.8 1.6 0.2 0. ]\n",
      " [5.1 3.3 1.7 0.5 0. ]\n",
      " [5.4 3.9 1.7 0.4 0. ]\n",
      " [5.4 3.4 1.7 0.2 0. ]\n",
      " [5.7 3.8 1.7 0.3 0. ]\n",
      " [4.8 3.4 1.9 0.2 0. ]\n",
      " [5.1 3.8 1.9 0.4 0. ]\n",
      " [7.  3.2 4.7 1.4 1. ]\n",
      " [4.9 2.4 3.3 1.  1. ]\n",
      " [5.  2.3 3.3 1.  1. ]\n",
      " [5.  2.  3.5 1.  1. ]\n",
      " [5.7 2.6 3.5 1.  1. ]\n",
      " [5.6 2.9 3.6 1.3 1. ]\n",
      " [5.5 2.4 3.7 1.  1. ]\n",
      " [5.5 2.4 3.8 1.1 1. ]\n",
      " [5.2 2.7 3.9 1.4 1. ]\n",
      " [5.6 2.5 3.9 1.1 1. ]\n",
      " [5.8 2.7 3.9 1.2 1. ]\n",
      " [5.5 2.3 4.  1.3 1. ]\n",
      " [5.5 2.5 4.  1.3 1. ]\n",
      " [5.8 2.6 4.  1.2 1. ]\n",
      " [6.  2.2 4.  1.  1. ]\n",
      " [6.1 2.8 4.  1.3 1. ]\n",
      " [5.6 3.  4.1 1.3 1. ]\n",
      " [5.8 2.7 4.1 1.  1. ]\n",
      " [5.6 2.7 4.2 1.3 1. ]\n",
      " [5.7 3.  4.2 1.2 1. ]\n",
      " [5.9 3.  4.2 1.5 1. ]\n",
      " [6.4 2.9 4.3 1.3 1. ]\n",
      " [5.5 2.6 4.4 1.2 1. ]\n",
      " [6.3 2.3 4.4 1.3 1. ]\n",
      " [6.6 3.  4.4 1.4 1. ]\n",
      " [6.7 3.1 4.4 1.4 1. ]\n",
      " [5.4 3.  4.5 1.5 1. ]\n",
      " [5.6 3.  4.5 1.5 1. ]\n",
      " [5.7 2.8 4.5 1.3 1. ]\n",
      " [6.  2.9 4.5 1.5 1. ]\n",
      " [6.  3.4 4.5 1.6 1. ]\n",
      " [6.2 2.2 4.5 1.5 1. ]\n",
      " [6.4 3.2 4.5 1.5 1. ]\n",
      " [6.1 3.  4.6 1.4 1. ]\n",
      " [6.5 2.8 4.6 1.5 1. ]\n",
      " [6.6 2.9 4.6 1.3 1. ]\n",
      " [6.1 2.9 4.7 1.4 1. ]\n",
      " [6.1 2.8 4.7 1.2 1. ]\n",
      " [6.3 3.3 4.7 1.6 1. ]\n",
      " [6.7 3.1 4.7 1.5 1. ]\n",
      " [5.9 3.2 4.8 1.8 1. ]\n",
      " [6.8 2.8 4.8 1.4 1. ]\n",
      " [6.3 2.5 4.9 1.5 1. ]\n",
      " [6.9 3.1 4.9 1.5 1. ]\n",
      " [6.7 3.  5.  1.7 1. ]\n",
      " [6.  2.7 5.1 1.6 1. ]\n",
      " [4.9 2.5 4.5 1.7 2. ]\n",
      " [6.  3.  4.8 1.8 2. ]\n",
      " [6.2 2.8 4.8 1.8 2. ]\n",
      " [5.6 2.8 4.9 2.  2. ]\n",
      " [6.1 3.  4.9 1.8 2. ]\n",
      " [6.3 2.7 4.9 1.8 2. ]\n",
      " [5.7 2.5 5.  2.  2. ]\n",
      " [6.  2.2 5.  1.5 2. ]\n",
      " [6.3 2.5 5.  1.9 2. ]\n",
      " [5.8 2.8 5.1 2.4 2. ]\n",
      " [5.8 2.7 5.1 1.9 2. ]\n",
      " [5.9 3.  5.1 1.8 2. ]\n",
      " [6.3 2.8 5.1 1.5 2. ]\n",
      " [6.5 3.2 5.1 2.  2. ]\n",
      " [6.9 3.1 5.1 2.3 2. ]\n",
      " [6.5 3.  5.2 2.  2. ]\n",
      " [6.7 3.  5.2 2.3 2. ]\n",
      " [6.4 2.7 5.3 1.9 2. ]\n",
      " [6.4 3.2 5.3 2.3 2. ]\n",
      " [6.2 3.4 5.4 2.3 2. ]\n",
      " [6.9 3.1 5.4 2.1 2. ]\n",
      " [6.4 3.1 5.5 1.8 2. ]\n",
      " [6.5 3.  5.5 1.8 2. ]\n",
      " [6.8 3.  5.5 2.1 2. ]\n",
      " [6.1 2.6 5.6 1.4 2. ]\n",
      " [6.3 2.9 5.6 1.8 2. ]\n",
      " [6.3 3.4 5.6 2.4 2. ]\n",
      " [6.4 2.8 5.6 2.1 2. ]\n",
      " [6.4 2.8 5.6 2.2 2. ]\n",
      " [6.7 3.1 5.6 2.4 2. ]\n",
      " [6.7 3.3 5.7 2.1 2. ]\n",
      " [6.7 3.3 5.7 2.5 2. ]\n",
      " [6.9 3.2 5.7 2.3 2. ]\n",
      " [6.5 3.  5.8 2.2 2. ]\n",
      " [6.7 2.5 5.8 1.8 2. ]\n",
      " [7.2 3.  5.8 1.6 2. ]\n",
      " [6.8 3.2 5.9 2.3 2. ]\n",
      " [7.1 3.  5.9 2.1 2. ]\n",
      " [7.2 3.2 6.  1.8 2. ]\n",
      " [7.2 3.6 6.1 2.5 2. ]\n",
      " [7.4 2.8 6.1 1.9 2. ]\n",
      " [7.7 3.  6.1 2.3 2. ]\n",
      " [7.3 2.9 6.3 1.8 2. ]\n",
      " [7.9 3.8 6.4 2.  2. ]\n",
      " [7.6 3.  6.6 2.1 2. ]\n",
      " [7.7 3.8 6.7 2.2 2. ]\n",
      " [7.7 2.8 6.7 2.  2. ]\n",
      " [7.7 2.6 6.9 2.3 2. ]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# let's convert our dataframe to a numpy array, named A\n",
    "#    Our ML library, scikit-learn operates entirely on numpy arrays.\n",
    "#\n",
    "A = df_final.values    # .values gets the numpy array\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.6 3.6 1.  0.2 0. ]\n",
      " [4.3 3.  1.1 0.1 0. ]\n",
      " [5.  3.2 1.2 0.2 0. ]\n",
      " [5.8 4.  1.2 0.2 0. ]\n",
      " [4.4 3.  1.3 0.2 0. ]\n",
      " [4.4 3.2 1.3 0.2 0. ]\n",
      " [4.5 2.3 1.3 0.3 0. ]\n",
      " [4.7 3.2 1.3 0.2 0. ]\n",
      " [5.  3.5 1.3 0.3 0. ]\n",
      " [5.4 3.9 1.3 0.4 0. ]\n",
      " [5.5 3.5 1.3 0.2 0. ]\n",
      " [4.4 2.9 1.4 0.2 0. ]\n",
      " [4.6 3.4 1.4 0.3 0. ]\n",
      " [4.6 3.2 1.4 0.2 0. ]\n",
      " [4.8 3.  1.4 0.1 0. ]\n",
      " [4.8 3.  1.4 0.3 0. ]\n",
      " [4.9 3.  1.4 0.2 0. ]\n",
      " [5.  3.6 1.4 0.2 0. ]\n",
      " [5.  3.3 1.4 0.2 0. ]\n",
      " [5.1 3.5 1.4 0.2 0. ]\n",
      " [5.1 3.5 1.4 0.3 0. ]\n",
      " [5.2 3.4 1.4 0.2 0. ]\n",
      " [5.5 4.2 1.4 0.2 0. ]\n",
      " [4.6 3.1 1.5 0.2 0. ]\n",
      " [4.9 3.1 1.5 0.1 0. ]\n",
      " [4.9 3.1 1.5 0.1 0. ]\n",
      " [4.9 3.1 1.5 0.1 0. ]\n",
      " [5.  3.4 1.5 0.2 0. ]\n",
      " [5.1 3.8 1.5 0.3 0. ]\n",
      " [5.1 3.7 1.5 0.4 0. ]\n",
      " [5.1 3.4 1.5 0.2 0. ]\n",
      " [5.2 3.5 1.5 0.2 0. ]\n",
      " [5.3 3.7 1.5 0.2 0. ]\n",
      " [5.4 3.7 1.5 0.2 0. ]\n",
      " [5.7 4.4 1.5 0.4 0. ]\n",
      " [4.7 3.2 1.6 0.2 0. ]\n",
      " [4.8 3.4 1.6 0.2 0. ]\n",
      " [5.  3.  1.6 0.2 0. ]\n",
      " [5.  3.4 1.6 0.4 0. ]\n",
      " [5.  3.5 1.6 0.6 0. ]\n",
      " [5.1 3.8 1.6 0.2 0. ]\n",
      " [5.1 3.3 1.7 0.5 0. ]\n",
      " [5.4 3.9 1.7 0.4 0. ]\n",
      " [5.4 3.4 1.7 0.2 0. ]\n",
      " [5.7 3.8 1.7 0.3 0. ]\n",
      " [4.8 3.4 1.9 0.2 0. ]\n",
      " [5.1 3.8 1.9 0.4 0. ]\n",
      " [7.  3.2 4.7 1.4 1. ]\n",
      " [4.9 2.4 3.3 1.  1. ]\n",
      " [5.  2.3 3.3 1.  1. ]\n",
      " [5.  2.  3.5 1.  1. ]\n",
      " [5.7 2.6 3.5 1.  1. ]\n",
      " [5.6 2.9 3.6 1.3 1. ]\n",
      " [5.5 2.4 3.7 1.  1. ]\n",
      " [5.5 2.4 3.8 1.1 1. ]\n",
      " [5.2 2.7 3.9 1.4 1. ]\n",
      " [5.6 2.5 3.9 1.1 1. ]\n",
      " [5.8 2.7 3.9 1.2 1. ]\n",
      " [5.5 2.3 4.  1.3 1. ]\n",
      " [5.5 2.5 4.  1.3 1. ]\n",
      " [5.8 2.6 4.  1.2 1. ]\n",
      " [6.  2.2 4.  1.  1. ]\n",
      " [6.1 2.8 4.  1.3 1. ]\n",
      " [5.6 3.  4.1 1.3 1. ]\n",
      " [5.8 2.7 4.1 1.  1. ]\n",
      " [5.6 2.7 4.2 1.3 1. ]\n",
      " [5.7 3.  4.2 1.2 1. ]\n",
      " [5.9 3.  4.2 1.5 1. ]\n",
      " [6.4 2.9 4.3 1.3 1. ]\n",
      " [5.5 2.6 4.4 1.2 1. ]\n",
      " [6.3 2.3 4.4 1.3 1. ]\n",
      " [6.6 3.  4.4 1.4 1. ]\n",
      " [6.7 3.1 4.4 1.4 1. ]\n",
      " [5.4 3.  4.5 1.5 1. ]\n",
      " [5.6 3.  4.5 1.5 1. ]\n",
      " [5.7 2.8 4.5 1.3 1. ]\n",
      " [6.  2.9 4.5 1.5 1. ]\n",
      " [6.  3.4 4.5 1.6 1. ]\n",
      " [6.2 2.2 4.5 1.5 1. ]\n",
      " [6.4 3.2 4.5 1.5 1. ]\n",
      " [6.1 3.  4.6 1.4 1. ]\n",
      " [6.5 2.8 4.6 1.5 1. ]\n",
      " [6.6 2.9 4.6 1.3 1. ]\n",
      " [6.1 2.9 4.7 1.4 1. ]\n",
      " [6.1 2.8 4.7 1.2 1. ]\n",
      " [6.3 3.3 4.7 1.6 1. ]\n",
      " [6.7 3.1 4.7 1.5 1. ]\n",
      " [5.9 3.2 4.8 1.8 1. ]\n",
      " [6.8 2.8 4.8 1.4 1. ]\n",
      " [6.3 2.5 4.9 1.5 1. ]\n",
      " [6.9 3.1 4.9 1.5 1. ]\n",
      " [6.7 3.  5.  1.7 1. ]\n",
      " [6.  2.7 5.1 1.6 1. ]\n",
      " [4.9 2.5 4.5 1.7 2. ]\n",
      " [6.  3.  4.8 1.8 2. ]\n",
      " [6.2 2.8 4.8 1.8 2. ]\n",
      " [5.6 2.8 4.9 2.  2. ]\n",
      " [6.1 3.  4.9 1.8 2. ]\n",
      " [6.3 2.7 4.9 1.8 2. ]\n",
      " [5.7 2.5 5.  2.  2. ]\n",
      " [6.  2.2 5.  1.5 2. ]\n",
      " [6.3 2.5 5.  1.9 2. ]\n",
      " [5.8 2.8 5.1 2.4 2. ]\n",
      " [5.8 2.7 5.1 1.9 2. ]\n",
      " [5.9 3.  5.1 1.8 2. ]\n",
      " [6.3 2.8 5.1 1.5 2. ]\n",
      " [6.5 3.2 5.1 2.  2. ]\n",
      " [6.9 3.1 5.1 2.3 2. ]\n",
      " [6.5 3.  5.2 2.  2. ]\n",
      " [6.7 3.  5.2 2.3 2. ]\n",
      " [6.4 2.7 5.3 1.9 2. ]\n",
      " [6.4 3.2 5.3 2.3 2. ]\n",
      " [6.2 3.4 5.4 2.3 2. ]\n",
      " [6.9 3.1 5.4 2.1 2. ]\n",
      " [6.4 3.1 5.5 1.8 2. ]\n",
      " [6.5 3.  5.5 1.8 2. ]\n",
      " [6.8 3.  5.5 2.1 2. ]\n",
      " [6.1 2.6 5.6 1.4 2. ]\n",
      " [6.3 2.9 5.6 1.8 2. ]\n",
      " [6.3 3.4 5.6 2.4 2. ]\n",
      " [6.4 2.8 5.6 2.1 2. ]\n",
      " [6.4 2.8 5.6 2.2 2. ]\n",
      " [6.7 3.1 5.6 2.4 2. ]\n",
      " [6.7 3.3 5.7 2.1 2. ]\n",
      " [6.7 3.3 5.7 2.5 2. ]\n",
      " [6.9 3.2 5.7 2.3 2. ]\n",
      " [6.5 3.  5.8 2.2 2. ]\n",
      " [6.7 2.5 5.8 1.8 2. ]\n",
      " [7.2 3.  5.8 1.6 2. ]\n",
      " [6.8 3.2 5.9 2.3 2. ]\n",
      " [7.1 3.  5.9 2.1 2. ]\n",
      " [7.2 3.2 6.  1.8 2. ]\n",
      " [7.2 3.6 6.1 2.5 2. ]\n",
      " [7.4 2.8 6.1 1.9 2. ]\n",
      " [7.7 3.  6.1 2.3 2. ]\n",
      " [7.3 2.9 6.3 1.8 2. ]\n",
      " [7.9 3.8 6.4 2.  2. ]\n",
      " [7.6 3.  6.6 2.1 2. ]\n",
      " [7.7 3.8 6.7 2.2 2. ]\n",
      " [7.7 2.8 6.7 2.  2. ]\n",
      " [7.7 2.6 6.9 2.3 2. ]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# let's make sure it's all floating-point, so we can multiply and divide\n",
    "#\n",
    "A = A.astype('float64')  # so many:  www.tutorialspoint.com/numpy/numpy_data_types.htm\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The dataset has 141 rows and 5 cols\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# nice to have NUM_ROWS and NUM_COLS around\n",
    "#\n",
    "NUM_ROWS, NUM_COLS = A.shape\n",
    "print(f\"\\nThe dataset has {NUM_ROWS} rows and {NUM_COLS} cols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flower #132 is [7.2 3.6 6.1 2.5 2. ]\n",
      "  Its sepallen is 7.2\n",
      "  Its sepalwid is 3.6\n",
      "  Its petallen is 6.1\n",
      "  Its petalwid is 2.5\n",
      "  Its irisname is virginica (2)\n"
     ]
    }
   ],
   "source": [
    "# let's use all of our variables, to reinforce names...\n",
    "\n",
    "# choose a row index, n:\n",
    "n = 132\n",
    "print(f\"flower #{n} is {A[n]}\")\n",
    "\n",
    "for i in range(len(COLUMNS)):\n",
    "    colname = COLUMNS[i]\n",
    "    if colname != 'irisname':\n",
    "        print(f\"  Its {colname} is {A[n][i]}\")\n",
    "    else:\n",
    "        species_num = int(A[n][i])\n",
    "        species = SPECIES[species_num]\n",
    "        print(f\"  Its {colname} is {species} ({species_num})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# we could write-our-own, but we don't have to! Let's \"library\"! After all,\n",
    "#\n",
    "#     the representation and storage for the trees is a big task\n",
    "#     we want an already-debugged algorithm!\n",
    "#     we want to ask q'ns about irises and how \"classifiable\" they are, \n",
    "#        rather than questions about implementation (at least for this moment...)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Start of data definitions +++\n",
      "\n",
      "X_all (just features: 10 rows) is \n",
      " [[4.6 3.6 1.  0.2]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [5.4 3.9 1.3 0.4]]\n",
      "y_all (just labels)   is \n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "print(\"+++ Start of data definitions +++\\n\")\n",
    "\n",
    "X_all = A[:,0:4].copy()  # X (features) ... is all rows, columns 0, 1, 2, 3\n",
    "y_all = A[:,4].copy()    # y (labels) ... is all rows, column 4 only\n",
    "\n",
    "print(f\"X_all (just features: 10 rows) is \\n {X_all[:10,:]}\")\n",
    "print(f\"y_all (just labels)   is \\n {y_all}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features\n",
      " [[6.4 3.1 5.5 1.8]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [5.1 3.5 1.4 0.2]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [5.  3.  1.6 0.2]\n",
      " [4.6 3.6 1.  0.2]]\n",
      "labels\n",
      " [2. 1. 2. 2. 0. 1. 0. 1. 0. 0. 1. 1. 2. 2. 2. 0. 2. 2. 1. 2. 2. 2. 2. 2.\n",
      " 0. 0. 0. 0. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2. 0. 2. 2. 2. 1. 1. 0. 1. 0. 1.\n",
      " 1. 0. 1. 1. 1. 2. 0. 0. 2. 0. 2. 2. 2. 2. 0. 1. 2. 1. 1. 1. 1. 1. 0. 0.\n",
      " 0. 0. 2. 0. 0. 1. 1. 2. 2. 1. 0. 0. 0. 0. 1. 1. 2. 1. 1. 2. 1. 2. 0. 0.\n",
      " 2. 2. 1. 0. 0. 0. 0. 1. 1. 2. 2. 1. 0. 0. 2. 0. 2. 0. 2. 0. 0. 1. 2. 1.\n",
      " 0. 2. 1. 1. 0. 1. 0. 2. 1. 2. 0. 1. 2. 1. 1. 2. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# we scramble the data, to give a different TRAIN/TEST split each time...\n",
    "# \n",
    "indices = np.random.permutation(len(y_all))  # indices is a permutation-list\n",
    "\n",
    "# we scramble both X and y, necessarily with the same permutation\n",
    "X_all = X_all[indices]              # we apply the _same_ permutation to each!\n",
    "y_all = y_all[indices]              # again...\n",
    "print(\"features\\n\", X_all[:10,:])\n",
    "print(\"labels\\n\",y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with 113 rows;  testing with 28 rows\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# We next separate into test data and training data ... \n",
    "#    + We will train on the training data...\n",
    "#    + We will _not_ look at the testing data to build the model\n",
    "#\n",
    "# Then, afterward, we will test on the testing data -- and see how well we do!\n",
    "#\n",
    "\n",
    "#\n",
    "# a common convention:  train on 80%, test on 20%    Let's define the TEST_PERCENT\n",
    "#\n",
    "NUM_ROWS = X_all.shape[0]     # the number of rows\n",
    "TEST_PERCENT = 0.20\n",
    "TEST_SIZE = int(TEST_PERCENT*NUM_ROWS)   # no harm in rounding down\n",
    "\n",
    "X_test = X_all[:TEST_SIZE]    # first section are for testing\n",
    "y_test = y_all[:TEST_SIZE]\n",
    "\n",
    "X_train = X_all[TEST_SIZE:]   # all the rest are for training\n",
    "y_train = y_all[TEST_SIZE:]\n",
    "\n",
    "print(f\"training with {len(y_train)} rows;  testing with {len(y_test)} rows\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test is [2. 1. 2. 2. 0. 1. 0. 1. 0. 0. 1. 1. 2. 2. 2. 0. 2. 2. 1. 2. 2. 2. 2. 2.\n",
      " 0. 0. 0. 0.]\n",
      "y_train is [2. 1. 2. 1. 1. 2. 2. 2. 1. 2. 0. 2. 2. 2. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1.\n",
      " 1. 2. 0. 0. 2. 0. 2. 2. 2. 2. 0. 1. 2. 1. 1. 1. 1. 1. 0. 0. 0. 0. 2. 0.\n",
      " 0. 1. 1. 2. 2. 1. 0. 0. 0. 0. 1. 1. 2. 1. 1. 2. 1. 2. 0. 0. 2. 2. 1. 0.\n",
      " 0. 0. 0. 1. 1. 2. 2. 1. 0. 0. 2. 0. 2. 0. 2. 0. 0. 1. 2. 1. 0. 2. 1. 1.\n",
      " 0. 1. 0. 2. 1. 2. 0. 1. 2. 1. 1. 2. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(f\"y_test is {y_test}\")\n",
    "print(f\"y_train is {y_train}\")   # to \"get a visual\" on these...\n",
    "# print(X_test)\n",
    "# print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING the input scaler:\n",
      "\n",
      "X_train_scaled (first 5 rows): [[ 2.19823024 -0.03045335  1.6776493   1.25043753]\n",
      " [ 0.71524506 -0.25986861  0.34098145  0.17489469]\n",
      " [ 0.83882716 -0.03045335  1.21272135  1.38488039]\n",
      " [ 0.09733457  0.42837717  0.63156142  0.84710897]\n",
      " [ 0.09733457 -0.03045335  0.28286545  0.4437804 ]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# for most NNets, it's important to keep the feature values in the -1-to-1 range\n",
    "#    This is done through the \"StandardScaler\" in scikit-learn\n",
    "# \n",
    "USE_SCALER = True   # this variable is important! It tracks if we need to use the scaler...\n",
    "\n",
    "# we \"train the scaler\"  (computes the mean and standard deviation)\n",
    "if USE_SCALER == True:\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    # Scale with the training data! ave becomes 0; stdev becomes 1\n",
    "    scaler.fit(X_train)   \n",
    "    \n",
    "if USE_SCALER == True:    # if we're using the scaler, all inputs need to be scaled...\n",
    "    X_train_scaled = scaler.transform(X_train) # scale!\n",
    "    X_test_scaled = scaler.transform(X_test) # scale!\n",
    "    print(\"USING the input scaler:\\n\")\n",
    "else:\n",
    "    X_train_scaled = X_train.copy()  # not using the scaler\n",
    "    X_test_scaled = X_test.copy()  # not using the scaler\n",
    "    print(\"NOT using the input scaler:\\n\")\n",
    "    \n",
    "y_train_scaled = y_train  # the predicted/desired labels are not scaled\n",
    "y_test_scaled = y_test  # not using the scaler\n",
    "    \n",
    "# ascii_table(X_train_scaled,y_train_scaled)  # unweildy\n",
    "print(f\"X_train_scaled (first 5 rows): {X_train_scaled[:5,:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "++++++++++  TRAINING:  begin  +++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "++++++++++  TRAINING:   end  +++++++++++++++\n",
      "\n",
      "\n",
      "The (log) prediction error (the loss) is 0.05180243443839832\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#\n",
    "# Here's where you can change the number of hidden layers\n",
    "# and number of neurons!\n",
    "#\n",
    "nn_classifier = MLPClassifier(hidden_layer_sizes=(5,5), max_iter=400, activation=\"tanh\",\n",
    "                    solver='sgd', verbose=False, shuffle=True,\n",
    "                    random_state=None, # reproduceability!\n",
    "                    learning_rate_init=.1, learning_rate = 'adaptive')\n",
    "\n",
    "print(\"\\n\\n++++++++++  TRAINING:  begin  +++++++++++++++\\n\\n\")\n",
    "nn_classifier.fit(X_train_scaled, y_train_scaled)\n",
    "print(\"\\n\\n++++++++++  TRAINING:   end  +++++++++++++++\\n\\n\")\n",
    "\n",
    "print(f\"The (log) prediction error (the loss) is {nn_classifier.loss_}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     flower input  ->  pred   desr \n",
      "[0.71524506 0.19896191 1.03837337 0.84710897] ->   2      2              \n",
      "[1.45673765 0.42837717 0.57344542 0.30933754] ->   1      1              \n",
      "[1.20957346 0.42837717 1.27083734 1.51932325] ->   2      2              \n",
      "[-0.14982962 -1.17752966  0.7477934   1.11599468] ->   2      2              \n",
      "[-1.01490431  0.65779243 -1.34438236 -1.30397672] ->   0      0              \n",
      "[-0.52057592 -0.03045335  0.45721344  0.4437804 ] ->   1      1              \n",
      "[-0.89132221  1.11662295 -1.34438236 -1.30397672] ->   0      0              \n",
      "[-0.27341172 -0.25986861 -0.06583051  0.17489469] ->   1      1              \n",
      "[-1.01490431 -0.03045335 -1.22815037 -1.30397672] ->   0      0              \n",
      "[-1.50923271  1.34603821 -1.57684634 -1.30397672] ->   0      0              \n",
      "[ 0.22091667 -0.25986861  0.45721344  0.4437804 ] ->   1      1              \n",
      "[-0.27341172 -0.03045335  0.45721344  0.4437804 ] ->   1      1              \n",
      "[2.56897654 1.80486874 1.56141731 1.11599468] ->   2      2              \n",
      "[0.59166297 0.88720769 1.09648936 1.6537661 ] ->   2      2              \n",
      "[ 2.32181234 -0.48928388  1.73576529  1.11599468] ->   2      2              \n",
      "[-1.01490431  0.88720769 -1.22815037 -1.03509101] ->   0      0              \n",
      "[ 0.22091667 -0.03045335  0.63156142  0.84710897] ->   2      2              \n",
      "[ 1.95106605 -0.48928388  1.38706933  0.98155182] ->   2      2              \n",
      "[ 0.96240926 -0.03045335  0.39909744  0.30933754] ->   1      1              \n",
      "[1.08599136 0.19896191 1.09648936 1.6537661 ] ->   2      2              \n",
      "[2.32181234 1.80486874 1.73576529 1.38488039] ->   2      2              \n",
      "[ 0.71524506 -0.71869914  0.92214138  0.98155182] ->   2      2              \n",
      "[ 0.09733457 -0.03045335  0.8059094   0.84710897] ->   2      2              \n",
      "[0.71524506 0.42837717 0.92214138 1.51932325] ->   2      2              \n",
      "[-1.01490431  1.34603821 -1.34438236 -1.30397672] ->   0      0              \n",
      "[-1.01490431  1.11662295 -1.40249836 -1.16953387] ->   0      0              \n",
      "[-1.26206851  0.88720769 -1.22815037 -1.30397672] ->   0      0              \n",
      "[-0.39699382  2.72252978 -1.34438236 -1.30397672] ->   0      0              \n",
      "\n",
      "correct predictions: 28 out of 28\n",
      "\n",
      "\n",
      "+++++ parameters, weights, etc. +++++\n",
      "\n",
      "\n",
      "weights/coefficients:\n",
      "\n",
      "[[-1.75404937 -0.75348188  0.17248849 -0.78518314]\n",
      " [ 2.02597199  0.89119138  0.01290878  0.97679192]\n",
      " [-1.8313514  -0.68939382  0.65523024 -0.71254168]]\n",
      "[[-1.7386689 ]\n",
      " [ 1.60021224]\n",
      " [-0.5215019 ]\n",
      " [ 1.49590365]]\n",
      "\n",
      "intercepts: [array([ 0.75617772,  1.03275051,  0.17818079, -0.5712695 ]), array([0.73343015])]\n",
      "\n",
      "all parameters: {'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (4,), 'learning_rate': 'adaptive', 'learning_rate_init': 0.1, 'max_fun': 15000, 'max_iter': 200, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': None, 'shuffle': True, 'solver': 'sgd', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# how did it do on the testing data?\n",
    "#\n",
    "\n",
    "def ascii_table_for_classifier(X,y,nn):\n",
    "    \"\"\" a table including predictions using nn.predict \"\"\"\n",
    "    predictions = nn.predict(X) # all predictions\n",
    "    prediction_probs = nn.predict_proba(X) # all prediction probabilities\n",
    "    # count correct\n",
    "    num_correct = 0\n",
    "    # printing\n",
    "    print(f\"{'flower input ':>18s} -> {'pred':^6s} {'desr':^6s}\") \n",
    "    for i in range(len(y)):\n",
    "        pred = predictions[i]\n",
    "        pred_probs = prediction_probs[i,:]\n",
    "        desired = y[i]\n",
    "        if pred != desired: result = \"  incorrect: \" + str(pred_probs)\n",
    "        else: result = \"\"; num_correct += 1\n",
    "        # can \"cheat\" in our view by using X_train here...\n",
    "        print(f\"{X[i,:]!s:>18s} -> {pred:^6.0f} {desired:^6.0f} {result:^10s}\") \n",
    "    print(f\"\\ncorrect predictions: {num_correct} out of {len(y)}\")\n",
    "    \n",
    "\n",
    "\n",
    "#\n",
    "# let's see how it did on the test data (also the training data!)\n",
    "#\n",
    "ascii_table_for_classifier(X_test_scaled,\n",
    "                           y_test_scaled,\n",
    "                           nn_classifier)   # this is our own f'n, above\n",
    "#\n",
    "# other things...\n",
    "#\n",
    "\n",
    "# nn = nn_classifier  # less to type?\n",
    "print(\"\\n\\n+++++ parameters, weights, etc. +++++\\n\")\n",
    "print(f\"\\nweights/coefficients:\\n\")\n",
    "for wts in nn.coefs_:\n",
    "    print(wts)\n",
    "print(f\"\\nintercepts: {nn.intercepts_}\")\n",
    "print(f\"\\nall parameters: {nn.get_params()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# we could use cross-validation to find the \"right\"/\"best\" size and shape of the NNet...\n",
    "#\n",
    "\n",
    "#\n",
    "# but, instead, lets explore a new direction: exploring feature-predictability\n",
    "#\n",
    "\n",
    "#\n",
    "# specifically, here is an example in which we estimate petalwid \n",
    "#               from the other four columns -- including the species!\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepallen</th>\n",
       "      <th>sepalwid</th>\n",
       "      <th>petallen</th>\n",
       "      <th>petalwid</th>\n",
       "      <th>irisname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>7.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepallen  sepalwid  petallen  petalwid  irisname\n",
       "0         4.6       3.6       1.0       0.2         0\n",
       "1         4.3       3.0       1.1       0.1         0\n",
       "2         5.0       3.2       1.2       0.2         0\n",
       "3         5.8       4.0       1.2       0.2         0\n",
       "4         4.4       3.0       1.3       0.2         0\n",
       "..        ...       ...       ...       ...       ...\n",
       "136       7.9       3.8       6.4       2.0         2\n",
       "137       7.6       3.0       6.6       2.1         2\n",
       "138       7.7       3.8       6.7       2.2         2\n",
       "139       7.7       2.8       6.7       2.0         2\n",
       "140       7.7       2.6       6.9       2.3         2\n",
       "\n",
       "[141 rows x 5 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Start of regression experiments... +++\n",
      "\n",
      "X_all (just features: 10 rows) is \n",
      " [[4.6 3.6 1.  0. ]\n",
      " [4.3 3.  1.1 0. ]\n",
      " [5.  3.2 1.2 0. ]\n",
      " [5.8 4.  1.2 0. ]\n",
      " [4.4 3.  1.3 0. ]\n",
      " [4.4 3.2 1.3 0. ]\n",
      " [4.5 2.3 1.3 0. ]\n",
      " [4.7 3.2 1.3 0. ]\n",
      " [5.  3.5 1.3 0. ]\n",
      " [5.4 3.9 1.3 0. ]]\n",
      "y_all (just petallen)   is \n",
      " [0.2 0.1 0.2 0.2 0.2 0.2 0.3 0.2 0.3 0.4 0.2 0.2 0.3 0.2 0.1 0.3 0.2 0.2\n",
      " 0.2 0.2 0.3 0.2 0.2 0.2 0.1 0.1 0.1 0.2 0.3 0.4 0.2 0.2 0.2 0.2 0.4 0.2\n",
      " 0.2 0.2 0.4 0.6 0.2 0.5 0.4 0.2 0.3 0.2 0.4 1.4 1.  1.  1.  1.  1.3 1.\n",
      " 1.1 1.4 1.1 1.2 1.3 1.3 1.2 1.  1.3 1.3 1.  1.3 1.2 1.5 1.3 1.2 1.3 1.4\n",
      " 1.4 1.5 1.5 1.3 1.5 1.6 1.5 1.5 1.4 1.5 1.3 1.4 1.2 1.6 1.5 1.8 1.4 1.5\n",
      " 1.5 1.7 1.6 1.7 1.8 1.8 2.  1.8 1.8 2.  1.5 1.9 2.4 1.9 1.8 1.5 2.  2.3\n",
      " 2.  2.3 1.9 2.3 2.3 2.1 1.8 1.8 2.1 1.4 1.8 2.4 2.1 2.2 2.4 2.1 2.5 2.3\n",
      " 2.2 1.8 1.6 2.3 2.1 1.8 2.5 1.9 2.3 1.8 2.  2.1 2.2 2.  2.3]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# regression model for iris petal widths (petalwid, column index 3) \n",
    "#\n",
    "\n",
    "print(\"+++ Start of regression experiments... +++\\n\")\n",
    "\n",
    "X_all = np.concatenate( (A[:,0:3], A[:,4:]),axis=1)  # columns 0, 1, 2, _not_3, and 4\n",
    "y_all = A[:,3]             # y (labels) ... is all rows, column 3 (petalwid) only\n",
    "\n",
    "print(f\"X_all (just features: 10 rows) is \\n {X_all[:10,:]}\")\n",
    "print(f\"y_all (just petallen)   is \\n {y_all}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features\n",
      " [[6.8 3.2 5.9 2. ]\n",
      " [5.6 3.  4.5 1. ]\n",
      " [5.  3.5 1.3 0. ]\n",
      " [6.4 3.2 5.3 2. ]\n",
      " [7.7 3.8 6.7 2. ]\n",
      " [6.2 3.4 5.4 2. ]\n",
      " [5.4 3.  4.5 1. ]\n",
      " [6.1 2.6 5.6 2. ]\n",
      " [6.9 3.1 5.1 2. ]\n",
      " [5.5 2.5 4.  1. ]]\n",
      "labels (petallen)\n",
      " [2.3 1.5 0.3 2.3 2.2 2.3 1.5 1.4 2.3 1.3 2.1 1.3 0.2 2.4 1.3 1.7 1.8 1.8\n",
      " 0.2 0.2 1.8 0.3 0.2 1.8 0.1 1.6 0.2 1.5 1.5 1.1 0.2 1.3 2.2 2.  2.1 0.3\n",
      " 1.5 2.1 0.2 0.4 1.4 2.1 1.2 0.2 2.3 1.2 2.4 0.2 2.  0.4 1.3 1.4 1.6 2.\n",
      " 0.2 0.1 0.2 0.4 1.5 0.1 2.2 2.1 1.8 0.2 1.8 1.4 1.9 0.2 1.6 1.  1.3 0.4\n",
      " 1.  2.  2.3 2.4 1.1 0.2 1.5 1.  0.2 1.8 0.2 0.3 0.2 1.4 1.9 1.  0.2 1.3\n",
      " 1.  0.2 1.9 0.2 2.5 1.8 1.  1.5 1.7 1.2 1.5 0.1 1.3 1.6 0.3 1.4 0.1 1.8\n",
      " 0.6 1.2 0.2 1.5 0.2 2.3 0.3 1.3 2.1 0.3 0.4 0.5 2.3 1.4 2.5 0.2 2.  1.8\n",
      " 0.2 1.  0.2 1.2 1.8 0.4 1.3 0.2 1.9 1.4 1.5 2.  0.2 1.8 1.5]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# we scramble the data, to give a different TRAIN/TEST split each time...\n",
    "# \n",
    "indices = np.random.permutation(len(y_all))  # indices is a permutation-list\n",
    "\n",
    "# we scramble both X and y, necessarily with the same permutation\n",
    "X_all = X_all[indices]              # we apply the _same_ permutation to each!\n",
    "y_all = y_all[indices]              # again...\n",
    "print(\"features\\n\", X_all[:10,:])\n",
    "print(\"labels (petallen)\\n\",y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with 113 rows;  testing with 28 rows\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# We next separate into test data and training data ... \n",
    "#    + We will train on the training data...\n",
    "#    + We will _not_ look at the testing data to build the model\n",
    "#\n",
    "# Then, afterward, we will test on the testing data -- and see how well we do!\n",
    "#\n",
    "\n",
    "#\n",
    "# a common convention:  train on 80%, test on 20%    Let's define the TEST_PERCENT\n",
    "#\n",
    "NUM_ROWS = X_all.shape[0]     # the number of rows\n",
    "TEST_PERCENT = 0.20\n",
    "TEST_SIZE = int(TEST_PERCENT*NUM_ROWS)   # no harm in rounding down\n",
    "\n",
    "X_test = X_all[:TEST_SIZE]    # first section are for testing\n",
    "y_test = y_all[:TEST_SIZE]\n",
    "\n",
    "X_train = X_all[TEST_SIZE:]   # all the rest are for training\n",
    "y_train = y_all[TEST_SIZE:]\n",
    "\n",
    "print(f\"training with {len(y_train)} rows;  testing with {len(y_test)} rows\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test is [2.3 1.5 0.3 2.3 2.2 2.3 1.5 1.4 2.3 1.3 2.1 1.3 0.2 2.4 1.3 1.7 1.8 1.8\n",
      " 0.2 0.2 1.8 0.3 0.2 1.8 0.1 1.6 0.2 1.5]\n",
      "y_train is [1.5 1.1 0.2 1.3 2.2 2.  2.1 0.3 1.5 2.1 0.2 0.4 1.4 2.1 1.2 0.2 2.3 1.2\n",
      " 2.4 0.2 2.  0.4 1.3 1.4 1.6 2.  0.2 0.1 0.2 0.4 1.5 0.1 2.2 2.1 1.8 0.2\n",
      " 1.8 1.4 1.9 0.2 1.6 1.  1.3 0.4 1.  2.  2.3 2.4 1.1 0.2 1.5 1.  0.2 1.8\n",
      " 0.2 0.3 0.2 1.4 1.9 1.  0.2 1.3 1.  0.2 1.9 0.2 2.5 1.8 1.  1.5 1.7 1.2\n",
      " 1.5 0.1 1.3 1.6 0.3 1.4 0.1 1.8 0.6 1.2 0.2 1.5 0.2 2.3 0.3 1.3 2.1 0.3\n",
      " 0.4 0.5 2.3 1.4 2.5 0.2 2.  1.8 0.2 1.  0.2 1.2 1.8 0.4 1.3 0.2 1.9 1.4\n",
      " 1.5 2.  0.2 1.8 1.5]\n"
     ]
    }
   ],
   "source": [
    "print(f\"y_test is {y_test}\")\n",
    "print(f\"y_train is {y_train}\")   # to \"get a visual\" on these...\n",
    "# print(X_test)\n",
    "# print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING the input scaler:\n",
      "\n",
      "X_train_scaled (first 5 rows): [[ 0.05003112 -0.11230034  0.27444923  0.05513514]\n",
      " [-0.42109523 -1.52229346  0.0480787   0.05513514]\n",
      " [-0.06775047  2.23768821 -1.42332976 -1.19091895]\n",
      " [ 0.87450222 -0.34729919  0.50081976  0.05513514]\n",
      " [ 0.63893905 -0.58229805  1.06674609  1.30118922]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# for most NNets, it's important to keep the feature values in the -1-to-1 range\n",
    "#    This is done through the \"StandardScaler\" in scikit-learn\n",
    "# \n",
    "USE_SCALER = True   # this variable is important! It tracks if we need to use the scaler...\n",
    "\n",
    "# we \"train the scaler\"  (computes the mean and standard deviation)\n",
    "if USE_SCALER == True:\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    # Scale with the training data! ave becomes 0; stdev becomes 1\n",
    "    scaler.fit(X_train)   \n",
    "    \n",
    "if USE_SCALER == True:    # if we're using the scaler, all inputs need to be scaled...\n",
    "    X_train_scaled = scaler.transform(X_train) # scale!\n",
    "    X_test_scaled = scaler.transform(X_test) # scale!\n",
    "    print(\"USING the input scaler:\\n\")\n",
    "else:\n",
    "    X_train_scaled = X_train.copy()  # not using the scaler\n",
    "    X_test_scaled = X_test.copy()  # not using the scaler\n",
    "    print(\"NOT using the input scaler:\\n\")\n",
    "    \n",
    "y_train_scaled = y_train  # the predicted/desired labels are not scaled\n",
    "y_test_scaled = y_test  # not using the scaler\n",
    "    \n",
    "# ascii_table(X_train_scaled,y_train_scaled)  # unweildy\n",
    "print(f\"X_train_scaled (first 5 rows): {X_train_scaled[:5,:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "++++++++++  TRAINING:  begin  +++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "++++++++++  TRAINING:   end  +++++++++++++++\n",
      "\n",
      "\n",
      "The (sq) prediction error (the loss) is 0.012638891574217543\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "#\n",
    "# Here's where you can change the number of hidden layers\n",
    "# and number of neurons!\n",
    "#\n",
    "nn_regressor = MLPRegressor(hidden_layer_sizes=(10,10), max_iter=400, activation=\"tanh\",\n",
    "                    solver='sgd', verbose=False, shuffle=True,\n",
    "                    random_state=None, # reproduceability!\n",
    "                    learning_rate_init=.1, learning_rate = 'adaptive')\n",
    "\n",
    "print(\"\\n\\n++++++++++  TRAINING:  begin  +++++++++++++++\\n\\n\")\n",
    "nn_regressor.fit(X_train_scaled, y_train_scaled)\n",
    "print(\"\\n\\n++++++++++  TRAINING:   end  +++++++++++++++\\n\\n\")\n",
    "\n",
    "print(f\"The (sq) prediction error (the loss) is {nn_regressor.loss_}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            input  ->   pred    desr    absdiff  \n",
      " [5.9 3.  4.2 1. ] ->  +2.162  +2.300    0.138   \n",
      " [5.5 2.4 3.8 1. ] ->  +1.388  +1.500    0.112   \n",
      " [5.8 4.  1.2 0. ] ->  +0.220  +0.300    0.080   \n",
      " [6.6 2.9 4.6 1. ] ->  +2.102  +2.300    0.198   \n",
      " [6.4 2.8 5.6 2. ] ->  +2.222  +2.200    0.022   \n",
      " [7.7 2.8 6.7 2. ] ->  +2.134  +2.300    0.166   \n",
      " [6.7 3.3 5.7 2. ] ->  +1.381  +1.500    0.119   \n",
      " [4.8 3.  1.4 0. ] ->  +2.005  +1.400    0.605   \n",
      " [6.2 2.2 4.5 1. ] ->  +2.036  +2.300    0.264   \n",
      " [6.9 3.1 5.4 2. ] ->  +1.212  +1.300    0.088   \n",
      " [5.5 3.5 1.3 0. ] ->  +2.072  +2.100    0.028   \n",
      " [5.1 3.8 1.9 0. ] ->  +1.300  +1.300    0.000   \n",
      " [6.1 2.9 4.7 1. ] ->  +0.248  +0.200    0.048   \n",
      " [7.6 3.  6.6 2. ] ->  +2.116  +2.400    0.284   \n",
      " [5.5 2.6 4.4 1. ] ->  +1.214  +1.300    0.086   \n",
      " [5.3 3.7 1.5 0. ] ->  +1.683  +1.700    0.017   \n",
      " [6.9 3.2 5.7 2. ] ->  +1.913  +1.800    0.113   \n",
      " [6.1 2.8 4.7 1. ] ->  +1.907  +1.800    0.107   \n",
      " [5.8 2.8 5.1 2. ] ->  +0.218  +0.200    0.018   \n",
      " [4.6 3.2 1.4 0. ] ->  +0.236  +0.200    0.036   \n",
      " [5.6 2.8 4.9 2. ] ->  +1.949  +1.800    0.149   \n",
      " [5.4 3.9 1.7 0. ] ->  +0.238  +0.300    0.062   \n",
      " [6.1 2.8 4.  1. ] ->  +0.250  +0.200    0.050   \n",
      " [7.  3.2 4.7 1. ] ->  +2.065  +1.800    0.265   \n",
      " [6.3 3.3 4.7 1. ] ->  +0.244  +0.100    0.144   \n",
      " [5.7 2.5 5.  2. ] ->  +1.463  +1.600    0.137   \n",
      " [5.  3.6 1.4 0. ] ->  +0.225  +0.200    0.025   \n",
      " [4.9 3.1 1.5 0. ] ->  +1.716  +1.500    0.216   \n",
      "\n",
      "average abs error: 0.12773929492225844\n",
      "\n",
      "\n",
      "+++++ parameters, weights, etc. +++++\n",
      "\n",
      "\n",
      "weights/coefficients:\n",
      "\n",
      "[[ 0.52370624  0.16477204 -0.21541661  0.29049751 -0.0272269  -0.17540813\n",
      "  -0.58963856 -0.97539167 -0.61322244  0.48723396]\n",
      " [ 0.53999875  0.25212957  0.09618546  0.1414002   0.17260656  0.2900656\n",
      "  -0.30714816  0.10012144 -0.60123445 -0.07215321]\n",
      " [ 0.26558362  0.80726729 -0.52036887  0.61902463 -0.67135939  0.11771537\n",
      "   0.14961286 -0.5411904   0.12572887 -0.37671501]\n",
      " [ 0.70472291  0.73872718  0.04974077  0.31216499 -0.74232844  0.42663622\n",
      "  -0.5520866   0.29206572 -0.96897729 -0.32841261]]\n",
      "[[-0.16813688  0.46695092  0.11356953 -0.06415481 -0.56430493 -0.14974477\n",
      "  -0.30649287  0.1124713  -0.4488791   0.1266132 ]\n",
      " [-0.16162809  0.21445251 -0.53124925 -0.06378483 -0.21168603 -0.42840198\n",
      "   0.13233988  0.56800658  0.4735135  -0.22033852]\n",
      " [-0.42663436  0.08463738  0.35514065  0.3097006  -0.04518288 -0.0012708\n",
      "   0.26409363  0.3386853   0.02571936 -0.05559333]\n",
      " [ 0.50618571  0.42326314 -0.5628321  -0.01575445 -0.0273194  -0.57533019\n",
      "  -0.00146445 -0.0864326  -0.17931242  0.1382968 ]\n",
      " [-0.4026413  -0.88630182  0.66506628  0.05699467  0.6536444   0.67264319\n",
      "  -0.35716198 -0.3398495  -0.05787763 -0.37442428]\n",
      " [ 0.39847337 -0.25264518 -0.11243803 -0.40105336 -0.03702863 -0.53270609\n",
      "  -0.443055    0.44386585  0.17114888  0.47241313]\n",
      " [ 0.44633209  0.59728734 -0.13013882 -0.10101027 -0.05385686  0.31692741\n",
      "   0.22348333 -0.23011857  0.30095733 -0.23055508]\n",
      " [ 0.22781611 -0.26783875  0.06077719  0.03608891 -0.55092266 -0.14100548\n",
      "   0.00570767 -0.15790641  0.34461583 -0.1333346 ]\n",
      " [-0.33879926  0.56384833  0.09761393  0.11614821 -0.64084461  0.27418003\n",
      "  -0.35432979  0.36110299 -0.36776689 -0.35309134]\n",
      " [ 0.17494104 -0.23228865 -0.0777509  -0.11832916  0.21885044  0.51027797\n",
      "  -0.33417817 -0.50323141 -0.08203286 -0.48029006]]\n",
      "[[ 0.56258387]\n",
      " [ 0.50527661]\n",
      " [ 0.20203432]\n",
      " [ 0.06906788]\n",
      " [ 0.02349392]\n",
      " [ 0.13275676]\n",
      " [-0.16137756]\n",
      " [ 0.5204033 ]\n",
      " [-0.33252723]\n",
      " [ 0.2471567 ]]\n",
      "\n",
      "intercepts: [array([ 0.09449259, -0.04459483,  0.34036957, -0.08435838, -0.18191613,\n",
      "       -0.02891853,  0.10723868,  0.04113836, -0.06966766,  0.50347566]), array([ 0.04175041,  0.35427318,  0.04403523, -0.83542753, -1.22854177,\n",
      "       -0.63002343, -0.12139113, -0.51697705, -0.13875792, -0.2071169 ]), array([1.26147881])]\n",
      "\n",
      "all parameters: {'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (10, 10), 'learning_rate': 'adaptive', 'learning_rate_init': 0.1, 'max_fun': 15000, 'max_iter': 400, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': None, 'shuffle': True, 'solver': 'sgd', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# how did it do? now we're making progress (by regressing)\n",
    "#\n",
    "\n",
    "def ascii_table_for_regressor(X,y,nn):\n",
    "    \"\"\" a table including predictions using nn.predict \"\"\"\n",
    "    predictions = nn.predict(X) # all predictions\n",
    "    # measure error\n",
    "    error = 0.0\n",
    "    # printing\n",
    "    print(f\"{'input ':>18s} ->  {'pred':^6s}  {'desr':^6s}  {'absdiff':^10s}\") \n",
    "    for i in range(len(y)):\n",
    "        pred = predictions[i]\n",
    "        desired = y[i]\n",
    "        result = abs(desired - pred)\n",
    "        error += result\n",
    "        # of use X_train to see the unscaled...\n",
    "        print(f\"{X_train[i,:]!s:>18s} ->  {pred:<+6.3f}  {desired:<+6.3f}  {result:^10.3f}\") \n",
    "    print(f\"\\naverage abs error: {error/len(y)}\")\n",
    "    \n",
    "\n",
    "\n",
    "#\n",
    "# let's see how it did on the test data (also the training data!)\n",
    "#\n",
    "ascii_table_for_regressor(X_test_scaled,\n",
    "                          y_test_scaled,\n",
    "                          nn_regressor)   # this is our own f'n, above\n",
    "#\n",
    "# other things...\n",
    "#\n",
    "nn = nn_regressor  # less to type?\n",
    "print(\"\\n\\n+++++ parameters, weights, etc. +++++\\n\")\n",
    "print(f\"\\nweights/coefficients:\\n\")\n",
    "for wts in nn.coefs_:\n",
    "    print(wts)\n",
    "print(f\"\\nintercepts: {nn.intercepts_}\")\n",
    "print(f\"\\nall parameters: {nn.get_params()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Your task!\n",
    "\n",
    "Just as above, find the average abs. error in the other three botanical features:\n",
    "  + sepalwid\n",
    "  + sepallen\n",
    "  + petalwid\n",
    "  + above is petallen, with a ~0.14 av. abs. error per petal! (pretty good :-)\n",
    "  \n",
    "Feel free to copy-and-paste a lot! \n",
    "  + You will feel like regressing is second nature (not a bad thing!)\n",
    "  \n",
    "_Or_, run a loop over the columns... (see the concatenate call, above)\n",
    "  + When you use concatenate, be sure all are slices, not single columns\n",
    "  \n",
    "Then, include a text (or markdown) cell with the four av. abs. errors\n",
    "  + Just as some features are more important than others,\n",
    "  + so, too are some features more predictable than others...\n",
    "  \n",
    "Not required...  How much do different network-shapes matter here?\n",
    "\n",
    "Not required... but also interesting:  \n",
    "  You could drop the species columns (species is now a feature), \n",
    "  in order to inquire how much \"value\" knowing species adds \n",
    "  when predicting each botanical measurement...\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepallen</th>\n",
       "      <th>sepalwid</th>\n",
       "      <th>petallen</th>\n",
       "      <th>petalwid</th>\n",
       "      <th>irisname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>7.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepallen  sepalwid  petallen  petalwid  irisname\n",
       "0         4.6       3.6       1.0       0.2         0\n",
       "1         4.3       3.0       1.1       0.1         0\n",
       "2         5.0       3.2       1.2       0.2         0\n",
       "3         5.8       4.0       1.2       0.2         0\n",
       "4         4.4       3.0       1.3       0.2         0\n",
       "..        ...       ...       ...       ...       ...\n",
       "136       7.9       3.8       6.4       2.0         2\n",
       "137       7.6       3.0       6.6       2.1         2\n",
       "138       7.7       3.8       6.7       2.2         2\n",
       "139       7.7       2.8       6.7       2.0         2\n",
       "140       7.7       2.6       6.9       2.3         2\n",
       "\n",
       "[141 rows x 5 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predicit Feature___sepallen\n",
    "\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.6 3.6 1.  0.2 0. ]\n",
      " [4.3 3.  1.1 0.1 0. ]\n",
      " [5.  3.2 1.2 0.2 0. ]\n",
      " [5.8 4.  1.2 0.2 0. ]\n",
      " [4.4 3.  1.3 0.2 0. ]\n",
      " [4.4 3.2 1.3 0.2 0. ]\n",
      " [4.5 2.3 1.3 0.3 0. ]\n",
      " [4.7 3.2 1.3 0.2 0. ]\n",
      " [5.  3.5 1.3 0.3 0. ]\n",
      " [5.4 3.9 1.3 0.4 0. ]\n",
      " [5.5 3.5 1.3 0.2 0. ]\n",
      " [4.4 2.9 1.4 0.2 0. ]\n",
      " [4.6 3.4 1.4 0.3 0. ]\n",
      " [4.6 3.2 1.4 0.2 0. ]\n",
      " [4.8 3.  1.4 0.1 0. ]\n",
      " [4.8 3.  1.4 0.3 0. ]\n",
      " [4.9 3.  1.4 0.2 0. ]\n",
      " [5.  3.6 1.4 0.2 0. ]\n",
      " [5.  3.3 1.4 0.2 0. ]\n",
      " [5.1 3.5 1.4 0.2 0. ]\n",
      " [5.1 3.5 1.4 0.3 0. ]\n",
      " [5.2 3.4 1.4 0.2 0. ]\n",
      " [5.5 4.2 1.4 0.2 0. ]\n",
      " [4.6 3.1 1.5 0.2 0. ]\n",
      " [4.9 3.1 1.5 0.1 0. ]\n",
      " [4.9 3.1 1.5 0.1 0. ]\n",
      " [4.9 3.1 1.5 0.1 0. ]\n",
      " [5.  3.4 1.5 0.2 0. ]\n",
      " [5.1 3.8 1.5 0.3 0. ]\n",
      " [5.1 3.7 1.5 0.4 0. ]\n",
      " [5.1 3.4 1.5 0.2 0. ]\n",
      " [5.2 3.5 1.5 0.2 0. ]\n",
      " [5.3 3.7 1.5 0.2 0. ]\n",
      " [5.4 3.7 1.5 0.2 0. ]\n",
      " [5.7 4.4 1.5 0.4 0. ]\n",
      " [4.7 3.2 1.6 0.2 0. ]\n",
      " [4.8 3.4 1.6 0.2 0. ]\n",
      " [5.  3.  1.6 0.2 0. ]\n",
      " [5.  3.4 1.6 0.4 0. ]\n",
      " [5.  3.5 1.6 0.6 0. ]\n",
      " [5.1 3.8 1.6 0.2 0. ]\n",
      " [5.1 3.3 1.7 0.5 0. ]\n",
      " [5.4 3.9 1.7 0.4 0. ]\n",
      " [5.4 3.4 1.7 0.2 0. ]\n",
      " [5.7 3.8 1.7 0.3 0. ]\n",
      " [4.8 3.4 1.9 0.2 0. ]\n",
      " [5.1 3.8 1.9 0.4 0. ]\n",
      " [7.  3.2 4.7 1.4 1. ]\n",
      " [4.9 2.4 3.3 1.  1. ]\n",
      " [5.  2.3 3.3 1.  1. ]\n",
      " [5.  2.  3.5 1.  1. ]\n",
      " [5.7 2.6 3.5 1.  1. ]\n",
      " [5.6 2.9 3.6 1.3 1. ]\n",
      " [5.5 2.4 3.7 1.  1. ]\n",
      " [5.5 2.4 3.8 1.1 1. ]\n",
      " [5.2 2.7 3.9 1.4 1. ]\n",
      " [5.6 2.5 3.9 1.1 1. ]\n",
      " [5.8 2.7 3.9 1.2 1. ]\n",
      " [5.5 2.3 4.  1.3 1. ]\n",
      " [5.5 2.5 4.  1.3 1. ]\n",
      " [5.8 2.6 4.  1.2 1. ]\n",
      " [6.  2.2 4.  1.  1. ]\n",
      " [6.1 2.8 4.  1.3 1. ]\n",
      " [5.6 3.  4.1 1.3 1. ]\n",
      " [5.8 2.7 4.1 1.  1. ]\n",
      " [5.6 2.7 4.2 1.3 1. ]\n",
      " [5.7 3.  4.2 1.2 1. ]\n",
      " [5.9 3.  4.2 1.5 1. ]\n",
      " [6.4 2.9 4.3 1.3 1. ]\n",
      " [5.5 2.6 4.4 1.2 1. ]\n",
      " [6.3 2.3 4.4 1.3 1. ]\n",
      " [6.6 3.  4.4 1.4 1. ]\n",
      " [6.7 3.1 4.4 1.4 1. ]\n",
      " [5.4 3.  4.5 1.5 1. ]\n",
      " [5.6 3.  4.5 1.5 1. ]\n",
      " [5.7 2.8 4.5 1.3 1. ]\n",
      " [6.  2.9 4.5 1.5 1. ]\n",
      " [6.  3.4 4.5 1.6 1. ]\n",
      " [6.2 2.2 4.5 1.5 1. ]\n",
      " [6.4 3.2 4.5 1.5 1. ]\n",
      " [6.1 3.  4.6 1.4 1. ]\n",
      " [6.5 2.8 4.6 1.5 1. ]\n",
      " [6.6 2.9 4.6 1.3 1. ]\n",
      " [6.1 2.9 4.7 1.4 1. ]\n",
      " [6.1 2.8 4.7 1.2 1. ]\n",
      " [6.3 3.3 4.7 1.6 1. ]\n",
      " [6.7 3.1 4.7 1.5 1. ]\n",
      " [5.9 3.2 4.8 1.8 1. ]\n",
      " [6.8 2.8 4.8 1.4 1. ]\n",
      " [6.3 2.5 4.9 1.5 1. ]\n",
      " [6.9 3.1 4.9 1.5 1. ]\n",
      " [6.7 3.  5.  1.7 1. ]\n",
      " [6.  2.7 5.1 1.6 1. ]\n",
      " [4.9 2.5 4.5 1.7 2. ]\n",
      " [6.  3.  4.8 1.8 2. ]\n",
      " [6.2 2.8 4.8 1.8 2. ]\n",
      " [5.6 2.8 4.9 2.  2. ]\n",
      " [6.1 3.  4.9 1.8 2. ]\n",
      " [6.3 2.7 4.9 1.8 2. ]\n",
      " [5.7 2.5 5.  2.  2. ]\n",
      " [6.  2.2 5.  1.5 2. ]\n",
      " [6.3 2.5 5.  1.9 2. ]\n",
      " [5.8 2.8 5.1 2.4 2. ]\n",
      " [5.8 2.7 5.1 1.9 2. ]\n",
      " [5.9 3.  5.1 1.8 2. ]\n",
      " [6.3 2.8 5.1 1.5 2. ]\n",
      " [6.5 3.2 5.1 2.  2. ]\n",
      " [6.9 3.1 5.1 2.3 2. ]\n",
      " [6.5 3.  5.2 2.  2. ]\n",
      " [6.7 3.  5.2 2.3 2. ]\n",
      " [6.4 2.7 5.3 1.9 2. ]\n",
      " [6.4 3.2 5.3 2.3 2. ]\n",
      " [6.2 3.4 5.4 2.3 2. ]\n",
      " [6.9 3.1 5.4 2.1 2. ]\n",
      " [6.4 3.1 5.5 1.8 2. ]\n",
      " [6.5 3.  5.5 1.8 2. ]\n",
      " [6.8 3.  5.5 2.1 2. ]\n",
      " [6.1 2.6 5.6 1.4 2. ]\n",
      " [6.3 2.9 5.6 1.8 2. ]\n",
      " [6.3 3.4 5.6 2.4 2. ]\n",
      " [6.4 2.8 5.6 2.1 2. ]\n",
      " [6.4 2.8 5.6 2.2 2. ]\n",
      " [6.7 3.1 5.6 2.4 2. ]\n",
      " [6.7 3.3 5.7 2.1 2. ]\n",
      " [6.7 3.3 5.7 2.5 2. ]\n",
      " [6.9 3.2 5.7 2.3 2. ]\n",
      " [6.5 3.  5.8 2.2 2. ]\n",
      " [6.7 2.5 5.8 1.8 2. ]\n",
      " [7.2 3.  5.8 1.6 2. ]\n",
      " [6.8 3.2 5.9 2.3 2. ]\n",
      " [7.1 3.  5.9 2.1 2. ]\n",
      " [7.2 3.2 6.  1.8 2. ]\n",
      " [7.2 3.6 6.1 2.5 2. ]\n",
      " [7.4 2.8 6.1 1.9 2. ]\n",
      " [7.7 3.  6.1 2.3 2. ]\n",
      " [7.3 2.9 6.3 1.8 2. ]\n",
      " [7.9 3.8 6.4 2.  2. ]\n",
      " [7.6 3.  6.6 2.1 2. ]\n",
      " [7.7 3.8 6.7 2.2 2. ]\n",
      " [7.7 2.8 6.7 2.  2. ]\n",
      " [7.7 2.6 6.9 2.3 2. ]]\n",
      "\n",
      "The dataset has 141 rows and 5 cols\n"
     ]
    }
   ],
   "source": [
    "A = df_final.values    \n",
    "# print(A)\n",
    "\n",
    "A = A.astype('float64') \n",
    "print(A)\n",
    "\n",
    "NUM_ROWS, NUM_COLS = A.shape\n",
    "print(f\"\\nThe dataset has {NUM_ROWS} rows and {NUM_COLS} cols\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Start of regression experiments... +++\n",
      "\n",
      "X_all (just features: 10 rows) is \n",
      " [[3.6 1.  0.2 0. ]\n",
      " [3.  1.1 0.1 0. ]\n",
      " [3.2 1.2 0.2 0. ]\n",
      " [4.  1.2 0.2 0. ]\n",
      " [3.  1.3 0.2 0. ]\n",
      " [3.2 1.3 0.2 0. ]\n",
      " [2.3 1.3 0.3 0. ]\n",
      " [3.2 1.3 0.2 0. ]\n",
      " [3.5 1.3 0.3 0. ]\n",
      " [3.9 1.3 0.4 0. ]]\n",
      "y_all (just petallen)   is \n",
      " [4.6 4.3 5.  5.8 4.4 4.4 4.5 4.7 5.  5.4 5.5 4.4 4.6 4.6 4.8 4.8 4.9 5.\n",
      " 5.  5.1 5.1 5.2 5.5 4.6 4.9 4.9 4.9 5.  5.1 5.1 5.1 5.2 5.3 5.4 5.7 4.7\n",
      " 4.8 5.  5.  5.  5.1 5.1 5.4 5.4 5.7 4.8 5.1 7.  4.9 5.  5.  5.7 5.6 5.5\n",
      " 5.5 5.2 5.6 5.8 5.5 5.5 5.8 6.  6.1 5.6 5.8 5.6 5.7 5.9 6.4 5.5 6.3 6.6\n",
      " 6.7 5.4 5.6 5.7 6.  6.  6.2 6.4 6.1 6.5 6.6 6.1 6.1 6.3 6.7 5.9 6.8 6.3\n",
      " 6.9 6.7 6.  4.9 6.  6.2 5.6 6.1 6.3 5.7 6.  6.3 5.8 5.8 5.9 6.3 6.5 6.9\n",
      " 6.5 6.7 6.4 6.4 6.2 6.9 6.4 6.5 6.8 6.1 6.3 6.3 6.4 6.4 6.7 6.7 6.7 6.9\n",
      " 6.5 6.7 7.2 6.8 7.1 7.2 7.2 7.4 7.7 7.3 7.9 7.6 7.7 7.7 7.7]\n"
     ]
    }
   ],
   "source": [
    "print(\"+++ Start of regression experiments... +++\\n\")\n",
    "\n",
    "X_all = A[:,1:].copy()  # columns 1, 2, 3, and 4\n",
    "y_all = A[:,0].copy()            # y (labels) ... is all rows, column 0 (sepallen) only\n",
    "\n",
    "print(f\"X_all (just features: 10 rows) is \\n {X_all[:10,:]}\")\n",
    "print(f\"y_all (just petallen)   is \\n {y_all}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features\n",
      " [[3.7 1.5 0.2 0. ]\n",
      " [3.  4.2 1.5 1. ]\n",
      " [3.1 1.5 0.1 0. ]\n",
      " [3.8 6.7 2.2 2. ]\n",
      " [2.7 4.1 1.  1. ]\n",
      " [2.3 4.  1.3 1. ]\n",
      " [2.9 4.7 1.4 1. ]\n",
      " [4.2 1.4 0.2 0. ]\n",
      " [2.6 4.4 1.2 1. ]\n",
      " [3.  5.  1.7 1. ]]\n",
      "labels (petallen)\n",
      " [5.3 5.9 4.9 7.7 5.8 5.5 6.1 5.5 5.5 6.7 6.9 6.5 5.7 5.1 5.7 5.6 5.6 5.4\n",
      " 6.4 5.5 5.1 4.8 6.4 5.1 5.8 6.  6.  4.9 4.4 7.7 5.  5.9 6.5 4.9 7.1 5.\n",
      " 5.5 6.3 6.3 6.3 4.6 7.9 5.1 7.3 6.7 4.3 6.7 5.6 6.  6.9 4.4 5.8 6.4 6.2\n",
      " 4.8 5.7 4.6 5.1 6.3 4.5 6.3 5.2 6.4 6.5 7.2 6.3 5.1 5.8 7.7 7.4 6.8 6.\n",
      " 6.7 6.6 6.3 5.7 5.6 7.  6.7 4.6 5.7 5.  6.7 5.8 5.8 7.7 5.  6.9 6.7 6.2\n",
      " 5.1 5.6 5.  5.  6.1 6.5 4.7 6.  4.8 7.6 5.2 5.2 6.2 6.7 5.  4.7 4.6 5.7\n",
      " 4.9 6.  5.  4.9 6.1 5.5 6.6 6.8 5.  4.4 5.9 5.4 6.9 5.4 5.6 6.8 6.1 5.4\n",
      " 7.2 6.1 6.3 6.4 5.1 6.4 5.  5.5 6.5 4.8 4.9 5.4 7.2 6.1 6.4]\n"
     ]
    }
   ],
   "source": [
    "# we scramble the data, to give a different TRAIN/TEST split each time...\n",
    "\n",
    "indices = np.random.permutation(len(y_all))  # indices is a permutation-list\n",
    "\n",
    "# we scramble both X and y, necessarily with the same permutation\n",
    "X_all = X_all[indices]              # we apply the _same_ permutation to each!\n",
    "y_all = y_all[indices]              # again...\n",
    "print(\"features\\n\", X_all[:10,:])\n",
    "print(\"labels (petallen)\\n\",y_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with 113 rows;  testing with 28 rows\n",
      "y_test is [5.3 5.9 4.9 7.7 5.8 5.5 6.1 5.5 5.5 6.7 6.9 6.5 5.7 5.1 5.7 5.6 5.6 5.4\n",
      " 6.4 5.5 5.1 4.8 6.4 5.1 5.8 6.  6.  4.9]\n",
      "y_train is [4.4 7.7 5.  5.9 6.5 4.9 7.1 5.  5.5 6.3 6.3 6.3 4.6 7.9 5.1 7.3 6.7 4.3\n",
      " 6.7 5.6 6.  6.9 4.4 5.8 6.4 6.2 4.8 5.7 4.6 5.1 6.3 4.5 6.3 5.2 6.4 6.5\n",
      " 7.2 6.3 5.1 5.8 7.7 7.4 6.8 6.  6.7 6.6 6.3 5.7 5.6 7.  6.7 4.6 5.7 5.\n",
      " 6.7 5.8 5.8 7.7 5.  6.9 6.7 6.2 5.1 5.6 5.  5.  6.1 6.5 4.7 6.  4.8 7.6\n",
      " 5.2 5.2 6.2 6.7 5.  4.7 4.6 5.7 4.9 6.  5.  4.9 6.1 5.5 6.6 6.8 5.  4.4\n",
      " 5.9 5.4 6.9 5.4 5.6 6.8 6.1 5.4 7.2 6.1 6.3 6.4 5.1 6.4 5.  5.5 6.5 4.8\n",
      " 4.9 5.4 7.2 6.1 6.4]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# a common convention:  train on 80%, test on 20%    Let's define the TEST_PERCENT\n",
    "#\n",
    "NUM_ROWS = X_all.shape[0]     # the number of rows\n",
    "TEST_PERCENT = 0.20\n",
    "TEST_SIZE = int(TEST_PERCENT*NUM_ROWS)   # no harm in rounding down\n",
    "\n",
    "X_test = X_all[:TEST_SIZE]    # first section are for testing\n",
    "y_test = y_all[:TEST_SIZE]\n",
    "\n",
    "X_train = X_all[TEST_SIZE:]   # all the rest are for training\n",
    "y_train = y_all[TEST_SIZE:]\n",
    "\n",
    "print(f\"training with {len(y_train)} rows;  testing with {len(y_test)} rows\" )\n",
    "\n",
    "print(f\"y_test is {y_test}\")\n",
    "print(f\"y_train is {y_train}\")   # to \"get a visual\" on these...\n",
    "# print(X_test)\n",
    "# print(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING the input scaler:\n",
      "\n",
      "X_train_scaled (first 5 rows): [[ 0.34360642 -1.3540539  -1.29014164 -1.21308122]\n",
      " [-0.12989999  1.2792872   1.40010978  1.15033564]\n",
      " [ 1.05386603 -1.3540539  -1.16203443 -1.21308122]\n",
      " [-0.12989999  0.73067448  0.75957372  1.15033564]\n",
      " [-0.12989999  0.95011957  0.75957372  1.15033564]]\n"
     ]
    }
   ],
   "source": [
    "# for most NNets, it's important to keep the feature values in the -1-to-1 range\n",
    "#    This is done through the \"StandardScaler\" in scikit-learn\n",
    "# \n",
    "USE_SCALER = True   # this variable is important! It tracks if we need to use the scaler...\n",
    "\n",
    "# we \"train the scaler\"  (computes the mean and standard deviation)\n",
    "if USE_SCALER == True:\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    # Scale with the training data! ave becomes 0; stdev becomes 1\n",
    "    scaler.fit(X_train)   \n",
    "    \n",
    "if USE_SCALER == True:    # if we're using the scaler, all inputs need to be scaled...\n",
    "    X_train_scaled = scaler.transform(X_train) # scale!\n",
    "    X_test_scaled = scaler.transform(X_test) # scale!\n",
    "    print(\"USING the input scaler:\\n\")\n",
    "else:\n",
    "    X_train_scaled = X_train.copy()  # not using the scaler\n",
    "    X_test_scaled = X_test.copy()  # not using the scaler\n",
    "    print(\"NOT using the input scaler:\\n\")\n",
    "    \n",
    "y_train_scaled = y_train  # the predicted/desired labels are not scaled\n",
    "y_test_scaled = y_test  # not using the scaler\n",
    "    \n",
    "# ascii_table(X_train_scaled,y_train_scaled)  # unweildy\n",
    "print(f\"X_train_scaled (first 5 rows): {X_train_scaled[:5,:]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "++++++++++  TRAINING:  begin  +++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "++++++++++  TRAINING:   end  +++++++++++++++\n",
      "\n",
      "\n",
      "The (sq) prediction error (the loss) is 0.03887807016517819\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "#\n",
    "# Here's where you can change the number of hidden layers\n",
    "# and number of neurons!\n",
    "#\n",
    "nn_regressor = MLPRegressor(hidden_layer_sizes=(10,10), max_iter=400, activation=\"tanh\",\n",
    "                    solver='sgd', verbose=False, shuffle=True,\n",
    "                    random_state=None, # reproduceability!\n",
    "                    learning_rate_init=.1, learning_rate = 'adaptive')\n",
    "\n",
    "print(\"\\n\\n++++++++++  TRAINING:  begin  +++++++++++++++\\n\\n\")\n",
    "nn_regressor.fit(X_train_scaled, y_train_scaled)\n",
    "print(\"\\n\\n++++++++++  TRAINING:   end  +++++++++++++++\\n\\n\")\n",
    "\n",
    "print(f\"The (sq) prediction error (the loss) is {nn_regressor.loss_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how did it do? now we're making progress (by regressing)\n",
    "#\n",
    "\n",
    "def ascii_table_for_regressor(X,y,nn):\n",
    "    \"\"\" a table including predictions using nn.predict \"\"\"\n",
    "    predictions = nn.predict(X) # all predictions\n",
    "    # measure error\n",
    "    error = 0.0\n",
    "    # printing\n",
    "    print(f\"{'input ':>18s} ->  {'pred':^6s}  {'desr':^6s}  {'absdiff':^10s}\") \n",
    "    for i in range(len(y)):\n",
    "        pred = predictions[i]\n",
    "        desired = y[i]\n",
    "        result = abs(desired - pred)\n",
    "        error += result\n",
    "        # of use X_train to see the unscaled...\n",
    "        print(f\"{X_train[i,:]!s:>18s} ->  {pred:<+6.3f}  {desired:<+6.3f}  {result:^10.3f}\") \n",
    "    print(f\"\\naverage abs error: {error/len(y)}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            input  ->   pred    desr    absdiff  \n",
      " [3.2 1.3 0.2 0. ] ->  +5.300  +5.300    0.000   \n",
      " [3.  6.1 2.3 2. ] ->  +6.017  +5.900    0.117   \n",
      " [3.5 1.3 0.3 0. ] ->  +4.718  +4.900    0.182   \n",
      " [3.  5.1 1.8 2. ] ->  +7.872  +7.700    0.172   \n",
      " [3.  5.5 1.8 2. ] ->  +5.778  +5.800    0.022   \n",
      " [3.1 1.5 0.1 0. ] ->  +5.668  +5.500    0.168   \n",
      " [3.  5.9 2.1 2. ] ->  +6.461  +6.100    0.361   \n",
      " [2.3 3.3 1.  1. ] ->  +5.591  +5.500    0.091   \n",
      " [2.4 3.8 1.1 1. ] ->  +6.157  +5.500    0.657   \n",
      " [3.3 4.7 1.6 1. ] ->  +6.654  +6.700    0.046   \n",
      " [3.4 5.6 2.4 2. ] ->  +6.755  +6.900    0.145   \n",
      " [2.9 5.6 1.8 2. ] ->  +6.821  +6.500    0.321   \n",
      " [3.2 1.4 0.2 0. ] ->  +6.262  +5.700    0.562   \n",
      " [3.8 6.4 2.  2. ] ->  +5.401  +5.100    0.301   \n",
      " [3.8 1.5 0.3 0. ] ->  +5.883  +5.700    0.183   \n",
      " [2.9 6.3 1.8 2. ] ->  +5.277  +5.600    0.323   \n",
      " [3.1 5.6 2.4 2. ] ->  +5.546  +5.600    0.054   \n",
      " [3.  1.1 0.1 0. ] ->  +6.276  +5.400    0.876   \n",
      " [2.5 5.8 1.8 2. ] ->  +6.225  +6.400    0.175   \n",
      " [2.8 4.9 2.  2. ] ->  +5.606  +5.500    0.106   \n",
      " [2.9 4.5 1.5 1. ] ->  +5.438  +5.100    0.338   \n",
      " [3.1 5.1 2.3 2. ] ->  +5.160  +4.800    0.360   \n",
      " [3.  1.3 0.2 0. ] ->  +6.334  +6.400    0.066   \n",
      " [4.  1.2 0.2 0. ] ->  +5.047  +5.100    0.053   \n",
      " [2.8 5.6 2.2 2. ] ->  +5.619  +5.800    0.181   \n",
      " [3.4 5.4 2.3 2. ] ->  +6.082  +6.000    0.082   \n",
      " [3.  1.4 0.1 0. ] ->  +6.786  +6.000    0.786   \n",
      " [3.  4.2 1.2 1. ] ->  +4.632  +4.900    0.268   \n",
      "\n",
      "average abs error: 0.2497869841636815\n",
      "\n",
      "\n",
      "+++++ parameters, weights, etc. +++++\n",
      "\n",
      "\n",
      "weights/coefficients:\n",
      "\n",
      "[[-0.24554894  0.10871327  0.35963188 -0.00562596 -0.7228616  -0.5853358\n",
      "  -0.16308668  0.24064141  0.4103243   0.82427674]\n",
      " [-0.43569264 -0.76100427 -0.87124115  1.28099971 -0.51976836  0.09473464\n",
      "  -1.99744564 -0.28767754  0.81154546 -1.72401918]\n",
      " [ 0.01363701 -0.02236005 -0.70484759 -0.19603299 -0.43338831 -0.9006723\n",
      "  -0.70110295  0.08993557  0.55569117 -0.88259266]\n",
      " [ 0.07172504 -0.0156439  -0.24055756 -0.28268434  0.03255833 -0.90355215\n",
      "  -0.87612189  0.21477885 -0.24187066 -0.76930692]]\n",
      "[[-2.91506494e-01 -5.10869804e-01  3.23927263e-01  1.31582769e-01\n",
      "  -1.81560679e-01  2.95524398e-01  1.82990879e-01 -9.58119232e-01\n",
      "  -5.13788879e-01  4.27214194e-01]\n",
      " [ 1.47304852e-01 -1.30433378e+00  1.55076986e+00  9.93694537e-01\n",
      "  -7.62141332e-02  1.90724951e-01  1.47443427e-01 -1.47454758e+00\n",
      "  -1.71577708e+00  9.44907743e-01]\n",
      " [-4.02642989e-01 -1.02960046e+00  1.30194366e+00  6.91884419e-01\n",
      "  -2.15147583e-01  1.03804437e-01 -7.01657924e-01 -5.30971840e-01\n",
      "  -1.62558997e+00  2.64403734e-01]\n",
      " [-4.89548736e-02  1.00430250e+00 -8.23994044e-01 -4.51586527e-01\n",
      "   3.24154397e-01 -9.05541614e-01  3.23471454e-01  1.02865556e+00\n",
      "   8.90511036e-01 -2.27565540e-01]\n",
      " [-1.76621719e-01 -1.91613316e-01  5.03286296e-01 -3.71351042e-01\n",
      "   1.60804755e-01  2.70292751e-02 -2.36184806e-01  2.66350461e-01\n",
      "  -7.26780698e-01 -6.93866503e-01]\n",
      " [ 2.68061728e-01  4.76462352e-01 -5.24265484e-01 -1.00646293e+00\n",
      "   3.71263294e-01 -4.00037168e-01  1.15802177e-01  1.55602396e+00\n",
      "   1.36558102e+00  3.93974471e-04]\n",
      " [ 1.55058360e-01 -1.44462179e-01  8.03707282e-01  1.10133865e+00\n",
      "  -3.60419469e-01  2.41967563e-01 -5.26087138e-01 -1.30348503e+00\n",
      "  -1.16738179e+00 -1.76412522e-01]\n",
      " [-9.02014025e-02  4.24975728e-01 -8.98784640e-01 -1.36337320e+00\n",
      "   4.25521213e-01  3.72275296e-01  2.14164984e-01  2.63264765e+00\n",
      "   1.98749054e+00 -8.94264452e-01]\n",
      " [-2.85917227e-01  1.18032871e+00 -1.01663707e+00 -3.01743133e-01\n",
      "   6.23967416e-02 -2.31757644e-01  1.19875193e-01  1.47388313e+00\n",
      "   1.17041027e+00 -6.24047497e-01]\n",
      " [-4.80630928e-01  1.39107405e-01 -1.20417621e-02 -3.90395638e-01\n",
      "  -2.98317331e-01  4.12344923e-01 -4.75514754e-01  3.24912138e-01\n",
      "   6.51933231e-01 -2.76049327e-01]]\n",
      "[[ 0.12080538]\n",
      " [ 0.72767249]\n",
      " [ 0.58459757]\n",
      " [ 2.77220386]\n",
      " [ 0.35976434]\n",
      " [-1.56133554]\n",
      " [-0.0293644 ]\n",
      " [ 1.17986971]\n",
      " [ 2.19696648]\n",
      " [ 0.76845747]]\n",
      "\n",
      "intercepts: [array([ 0.26212123,  0.67752283, -0.49873022, -1.15687015, -0.26162181,\n",
      "       -0.63714056, -2.24119934,  2.13241967, -0.49999857, -1.84622245]), array([ 0.51108236,  1.2636261 , -2.64187878, -3.10598576,  0.79751693,\n",
      "        0.04175433,  0.14177014,  4.37119308,  4.27089895, -0.81493763]), array([6.48625582])]\n",
      "\n",
      "all parameters: {'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (10, 10), 'learning_rate': 'adaptive', 'learning_rate_init': 0.1, 'max_fun': 15000, 'max_iter': 400, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': None, 'shuffle': True, 'solver': 'sgd', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "# let's see how it did on the test data (also the training data!)\n",
    "#\n",
    "ascii_table_for_regressor(X_test_scaled,\n",
    "                          y_test_scaled,\n",
    "                          nn_regressor)   # this is our own f'n, above\n",
    "#\n",
    "# other things...\n",
    "#\n",
    "nn = nn_regressor  # less to type?\n",
    "print(\"\\n\\n+++++ parameters, weights, etc. +++++\\n\")\n",
    "print(f\"\\nweights/coefficients:\\n\")\n",
    "for wts in nn.coefs_:\n",
    "    print(wts)\n",
    "print(f\"\\nintercepts: {nn.intercepts_}\")\n",
    "print(f\"\\nall parameters: {nn.get_params()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepallen</th>\n",
       "      <th>sepalwid</th>\n",
       "      <th>petallen</th>\n",
       "      <th>petalwid</th>\n",
       "      <th>irisname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>7.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepallen  sepalwid  petallen  petalwid  irisname\n",
       "0         4.6       3.6       1.0       0.2         0\n",
       "1         4.3       3.0       1.1       0.1         0\n",
       "2         5.0       3.2       1.2       0.2         0\n",
       "3         5.8       4.0       1.2       0.2         0\n",
       "4         4.4       3.0       1.3       0.2         0\n",
       "..        ...       ...       ...       ...       ...\n",
       "136       7.9       3.8       6.4       2.0         2\n",
       "137       7.6       3.0       6.6       2.1         2\n",
       "138       7.7       3.8       6.7       2.2         2\n",
       "139       7.7       2.8       6.7       2.0         2\n",
       "140       7.7       2.6       6.9       2.3         2\n",
       "\n",
       "[141 rows x 5 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predicit Feature___sepalwid\n",
    "\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.6 3.6 1.  0.2 0. ]\n",
      " [4.3 3.  1.1 0.1 0. ]\n",
      " [5.  3.2 1.2 0.2 0. ]\n",
      " [5.8 4.  1.2 0.2 0. ]\n",
      " [4.4 3.  1.3 0.2 0. ]\n",
      " [4.4 3.2 1.3 0.2 0. ]\n",
      " [4.5 2.3 1.3 0.3 0. ]\n",
      " [4.7 3.2 1.3 0.2 0. ]\n",
      " [5.  3.5 1.3 0.3 0. ]\n",
      " [5.4 3.9 1.3 0.4 0. ]\n",
      " [5.5 3.5 1.3 0.2 0. ]\n",
      " [4.4 2.9 1.4 0.2 0. ]\n",
      " [4.6 3.4 1.4 0.3 0. ]\n",
      " [4.6 3.2 1.4 0.2 0. ]\n",
      " [4.8 3.  1.4 0.1 0. ]\n",
      " [4.8 3.  1.4 0.3 0. ]\n",
      " [4.9 3.  1.4 0.2 0. ]\n",
      " [5.  3.6 1.4 0.2 0. ]\n",
      " [5.  3.3 1.4 0.2 0. ]\n",
      " [5.1 3.5 1.4 0.2 0. ]\n",
      " [5.1 3.5 1.4 0.3 0. ]\n",
      " [5.2 3.4 1.4 0.2 0. ]\n",
      " [5.5 4.2 1.4 0.2 0. ]\n",
      " [4.6 3.1 1.5 0.2 0. ]\n",
      " [4.9 3.1 1.5 0.1 0. ]\n",
      " [4.9 3.1 1.5 0.1 0. ]\n",
      " [4.9 3.1 1.5 0.1 0. ]\n",
      " [5.  3.4 1.5 0.2 0. ]\n",
      " [5.1 3.8 1.5 0.3 0. ]\n",
      " [5.1 3.7 1.5 0.4 0. ]\n",
      " [5.1 3.4 1.5 0.2 0. ]\n",
      " [5.2 3.5 1.5 0.2 0. ]\n",
      " [5.3 3.7 1.5 0.2 0. ]\n",
      " [5.4 3.7 1.5 0.2 0. ]\n",
      " [5.7 4.4 1.5 0.4 0. ]\n",
      " [4.7 3.2 1.6 0.2 0. ]\n",
      " [4.8 3.4 1.6 0.2 0. ]\n",
      " [5.  3.  1.6 0.2 0. ]\n",
      " [5.  3.4 1.6 0.4 0. ]\n",
      " [5.  3.5 1.6 0.6 0. ]\n",
      " [5.1 3.8 1.6 0.2 0. ]\n",
      " [5.1 3.3 1.7 0.5 0. ]\n",
      " [5.4 3.9 1.7 0.4 0. ]\n",
      " [5.4 3.4 1.7 0.2 0. ]\n",
      " [5.7 3.8 1.7 0.3 0. ]\n",
      " [4.8 3.4 1.9 0.2 0. ]\n",
      " [5.1 3.8 1.9 0.4 0. ]\n",
      " [7.  3.2 4.7 1.4 1. ]\n",
      " [4.9 2.4 3.3 1.  1. ]\n",
      " [5.  2.3 3.3 1.  1. ]\n",
      " [5.  2.  3.5 1.  1. ]\n",
      " [5.7 2.6 3.5 1.  1. ]\n",
      " [5.6 2.9 3.6 1.3 1. ]\n",
      " [5.5 2.4 3.7 1.  1. ]\n",
      " [5.5 2.4 3.8 1.1 1. ]\n",
      " [5.2 2.7 3.9 1.4 1. ]\n",
      " [5.6 2.5 3.9 1.1 1. ]\n",
      " [5.8 2.7 3.9 1.2 1. ]\n",
      " [5.5 2.3 4.  1.3 1. ]\n",
      " [5.5 2.5 4.  1.3 1. ]\n",
      " [5.8 2.6 4.  1.2 1. ]\n",
      " [6.  2.2 4.  1.  1. ]\n",
      " [6.1 2.8 4.  1.3 1. ]\n",
      " [5.6 3.  4.1 1.3 1. ]\n",
      " [5.8 2.7 4.1 1.  1. ]\n",
      " [5.6 2.7 4.2 1.3 1. ]\n",
      " [5.7 3.  4.2 1.2 1. ]\n",
      " [5.9 3.  4.2 1.5 1. ]\n",
      " [6.4 2.9 4.3 1.3 1. ]\n",
      " [5.5 2.6 4.4 1.2 1. ]\n",
      " [6.3 2.3 4.4 1.3 1. ]\n",
      " [6.6 3.  4.4 1.4 1. ]\n",
      " [6.7 3.1 4.4 1.4 1. ]\n",
      " [5.4 3.  4.5 1.5 1. ]\n",
      " [5.6 3.  4.5 1.5 1. ]\n",
      " [5.7 2.8 4.5 1.3 1. ]\n",
      " [6.  2.9 4.5 1.5 1. ]\n",
      " [6.  3.4 4.5 1.6 1. ]\n",
      " [6.2 2.2 4.5 1.5 1. ]\n",
      " [6.4 3.2 4.5 1.5 1. ]\n",
      " [6.1 3.  4.6 1.4 1. ]\n",
      " [6.5 2.8 4.6 1.5 1. ]\n",
      " [6.6 2.9 4.6 1.3 1. ]\n",
      " [6.1 2.9 4.7 1.4 1. ]\n",
      " [6.1 2.8 4.7 1.2 1. ]\n",
      " [6.3 3.3 4.7 1.6 1. ]\n",
      " [6.7 3.1 4.7 1.5 1. ]\n",
      " [5.9 3.2 4.8 1.8 1. ]\n",
      " [6.8 2.8 4.8 1.4 1. ]\n",
      " [6.3 2.5 4.9 1.5 1. ]\n",
      " [6.9 3.1 4.9 1.5 1. ]\n",
      " [6.7 3.  5.  1.7 1. ]\n",
      " [6.  2.7 5.1 1.6 1. ]\n",
      " [4.9 2.5 4.5 1.7 2. ]\n",
      " [6.  3.  4.8 1.8 2. ]\n",
      " [6.2 2.8 4.8 1.8 2. ]\n",
      " [5.6 2.8 4.9 2.  2. ]\n",
      " [6.1 3.  4.9 1.8 2. ]\n",
      " [6.3 2.7 4.9 1.8 2. ]\n",
      " [5.7 2.5 5.  2.  2. ]\n",
      " [6.  2.2 5.  1.5 2. ]\n",
      " [6.3 2.5 5.  1.9 2. ]\n",
      " [5.8 2.8 5.1 2.4 2. ]\n",
      " [5.8 2.7 5.1 1.9 2. ]\n",
      " [5.9 3.  5.1 1.8 2. ]\n",
      " [6.3 2.8 5.1 1.5 2. ]\n",
      " [6.5 3.2 5.1 2.  2. ]\n",
      " [6.9 3.1 5.1 2.3 2. ]\n",
      " [6.5 3.  5.2 2.  2. ]\n",
      " [6.7 3.  5.2 2.3 2. ]\n",
      " [6.4 2.7 5.3 1.9 2. ]\n",
      " [6.4 3.2 5.3 2.3 2. ]\n",
      " [6.2 3.4 5.4 2.3 2. ]\n",
      " [6.9 3.1 5.4 2.1 2. ]\n",
      " [6.4 3.1 5.5 1.8 2. ]\n",
      " [6.5 3.  5.5 1.8 2. ]\n",
      " [6.8 3.  5.5 2.1 2. ]\n",
      " [6.1 2.6 5.6 1.4 2. ]\n",
      " [6.3 2.9 5.6 1.8 2. ]\n",
      " [6.3 3.4 5.6 2.4 2. ]\n",
      " [6.4 2.8 5.6 2.1 2. ]\n",
      " [6.4 2.8 5.6 2.2 2. ]\n",
      " [6.7 3.1 5.6 2.4 2. ]\n",
      " [6.7 3.3 5.7 2.1 2. ]\n",
      " [6.7 3.3 5.7 2.5 2. ]\n",
      " [6.9 3.2 5.7 2.3 2. ]\n",
      " [6.5 3.  5.8 2.2 2. ]\n",
      " [6.7 2.5 5.8 1.8 2. ]\n",
      " [7.2 3.  5.8 1.6 2. ]\n",
      " [6.8 3.2 5.9 2.3 2. ]\n",
      " [7.1 3.  5.9 2.1 2. ]\n",
      " [7.2 3.2 6.  1.8 2. ]\n",
      " [7.2 3.6 6.1 2.5 2. ]\n",
      " [7.4 2.8 6.1 1.9 2. ]\n",
      " [7.7 3.  6.1 2.3 2. ]\n",
      " [7.3 2.9 6.3 1.8 2. ]\n",
      " [7.9 3.8 6.4 2.  2. ]\n",
      " [7.6 3.  6.6 2.1 2. ]\n",
      " [7.7 3.8 6.7 2.2 2. ]\n",
      " [7.7 2.8 6.7 2.  2. ]\n",
      " [7.7 2.6 6.9 2.3 2. ]]\n",
      "\n",
      "The dataset has 141 rows and 5 cols\n"
     ]
    }
   ],
   "source": [
    "A = df_final.values    \n",
    "# print(A)\n",
    "\n",
    "A = A.astype('float64') \n",
    "print(A)\n",
    "\n",
    "NUM_ROWS, NUM_COLS = A.shape\n",
    "print(f\"\\nThe dataset has {NUM_ROWS} rows and {NUM_COLS} cols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Start of regression experiments... +++\n",
      "\n",
      "X_all (just features: 10 rows) is \n",
      " [[4.6 1.  0.2 0. ]\n",
      " [4.3 1.1 0.1 0. ]\n",
      " [5.  1.2 0.2 0. ]\n",
      " [5.8 1.2 0.2 0. ]\n",
      " [4.4 1.3 0.2 0. ]\n",
      " [4.4 1.3 0.2 0. ]\n",
      " [4.5 1.3 0.3 0. ]\n",
      " [4.7 1.3 0.2 0. ]\n",
      " [5.  1.3 0.3 0. ]\n",
      " [5.4 1.3 0.4 0. ]]\n",
      "y_all (just petallen)   is \n",
      " [3.6 3.  3.2 4.  3.  3.2 2.3 3.2 3.5 3.9 3.5 2.9 3.4 3.2 3.  3.  3.  3.6\n",
      " 3.3 3.5 3.5 3.4 4.2 3.1 3.1 3.1 3.1 3.4 3.8 3.7 3.4 3.5 3.7 3.7 4.4 3.2\n",
      " 3.4 3.  3.4 3.5 3.8 3.3 3.9 3.4 3.8 3.4 3.8 3.2 2.4 2.3 2.  2.6 2.9 2.4\n",
      " 2.4 2.7 2.5 2.7 2.3 2.5 2.6 2.2 2.8 3.  2.7 2.7 3.  3.  2.9 2.6 2.3 3.\n",
      " 3.1 3.  3.  2.8 2.9 3.4 2.2 3.2 3.  2.8 2.9 2.9 2.8 3.3 3.1 3.2 2.8 2.5\n",
      " 3.1 3.  2.7 2.5 3.  2.8 2.8 3.  2.7 2.5 2.2 2.5 2.8 2.7 3.  2.8 3.2 3.1\n",
      " 3.  3.  2.7 3.2 3.4 3.1 3.1 3.  3.  2.6 2.9 3.4 2.8 2.8 3.1 3.3 3.3 3.2\n",
      " 3.  2.5 3.  3.2 3.  3.2 3.6 2.8 3.  2.9 3.8 3.  3.8 2.8 2.6]\n"
     ]
    }
   ],
   "source": [
    "print(\"+++ Start of regression experiments... +++\\n\")\n",
    "\n",
    "X_all = np.concatenate( (A[:,0:1], A[:,2:]),axis=1)  # columns 0, 2, 3, and 4\n",
    "y_all = A[:,1]             # y (labels) ... is all rows, column 1 (petalwid) only\n",
    "\n",
    "print(f\"X_all (just features: 10 rows) is \\n {X_all[:10,:]}\")\n",
    "print(f\"y_all (just petallen)   is \\n {y_all}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features\n",
      " [[5.1 1.5 0.3 0. ]\n",
      " [4.7 1.3 0.2 0. ]\n",
      " [4.8 1.6 0.2 0. ]\n",
      " [4.6 1.  0.2 0. ]\n",
      " [6.4 5.6 2.2 2. ]\n",
      " [5.6 4.2 1.3 1. ]\n",
      " [5.  1.5 0.2 0. ]\n",
      " [6.  5.  1.5 2. ]\n",
      " [4.6 1.4 0.2 0. ]\n",
      " [4.9 1.4 0.2 0. ]]\n",
      "labels (petallen)\n",
      " [3.8 3.2 3.4 3.6 2.8 2.7 3.4 2.2 3.2 3.  3.  3.8 2.7 4.  3.  2.  3.5 3.3\n",
      " 3.  3.5 3.4 2.6 3.2 2.9 2.7 2.5 3.7 3.7 2.8 2.7 2.3 3.5 3.8 3.4 3.1 3.4\n",
      " 3.  3.4 3.1 3.6 3.4 2.9 3.  3.2 3.1 3.2 3.  3.2 2.2 2.7 2.6 3.3 3.  3.4\n",
      " 2.5 3.  3.7 3.1 2.8 2.5 3.  2.3 3.  2.9 3.  2.3 3.8 2.7 2.7 3.2 3.4 3.6\n",
      " 2.8 3.2 2.5 2.2 3.  3.4 3.9 2.8 2.8 3.  3.1 3.5 2.4 3.  3.  2.7 2.9 3.\n",
      " 2.8 3.  2.8 2.8 3.1 3.4 3.2 3.1 2.3 3.2 3.1 3.5 3.5 2.8 3.  3.1 2.8 3.\n",
      " 3.3 2.9 3.2 2.4 3.1 3.3 2.4 3.2 3.8 2.5 3.  3.  2.9 2.8 2.5 3.  3.  3.3\n",
      " 2.6 2.5 3.9 3.2 3.1 2.8 3.  4.2 3.  4.4 2.9 2.9 2.6 3.8 2.6]\n"
     ]
    }
   ],
   "source": [
    "indices = np.random.permutation(len(y_all))  # indices is a permutation-list\n",
    "\n",
    "# we scramble both X and y, necessarily with the same permutation\n",
    "X_all = X_all[indices]              # we apply the _same_ permutation to each!\n",
    "y_all = y_all[indices]              # again...\n",
    "print(\"features\\n\", X_all[:10,:])\n",
    "print(\"labels (petallen)\\n\",y_all)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with 113 rows;  testing with 28 rows\n",
      "y_test is [3.8 3.2 3.4 3.6 2.8 2.7 3.4 2.2 3.2 3.  3.  3.8 2.7 4.  3.  2.  3.5 3.3\n",
      " 3.  3.5 3.4 2.6 3.2 2.9 2.7 2.5 3.7 3.7]\n",
      "y_train is [2.8 2.7 2.3 3.5 3.8 3.4 3.1 3.4 3.  3.4 3.1 3.6 3.4 2.9 3.  3.2 3.1 3.2\n",
      " 3.  3.2 2.2 2.7 2.6 3.3 3.  3.4 2.5 3.  3.7 3.1 2.8 2.5 3.  2.3 3.  2.9\n",
      " 3.  2.3 3.8 2.7 2.7 3.2 3.4 3.6 2.8 3.2 2.5 2.2 3.  3.4 3.9 2.8 2.8 3.\n",
      " 3.1 3.5 2.4 3.  3.  2.7 2.9 3.  2.8 3.  2.8 2.8 3.1 3.4 3.2 3.1 2.3 3.2\n",
      " 3.1 3.5 3.5 2.8 3.  3.1 2.8 3.  3.3 2.9 3.2 2.4 3.1 3.3 2.4 3.2 3.8 2.5\n",
      " 3.  3.  2.9 2.8 2.5 3.  3.  3.3 2.6 2.5 3.9 3.2 3.1 2.8 3.  4.2 3.  4.4\n",
      " 2.9 2.9 2.6 3.8 2.6]\n"
     ]
    }
   ],
   "source": [
    "# a common convention:  train on 80%, test on 20%    Let's define the TEST_PERCENT\n",
    "#\n",
    "NUM_ROWS = X_all.shape[0]     # the number of rows\n",
    "TEST_PERCENT = 0.20\n",
    "TEST_SIZE = int(TEST_PERCENT*NUM_ROWS)   # no harm in rounding down\n",
    "\n",
    "X_test = X_all[:TEST_SIZE]    # first section are for testing\n",
    "y_test = y_all[:TEST_SIZE]\n",
    "\n",
    "X_train = X_all[TEST_SIZE:]   # all the rest are for training\n",
    "y_train = y_all[TEST_SIZE:]\n",
    "\n",
    "print(f\"training with {len(y_train)} rows;  testing with {len(y_test)} rows\" )\n",
    "\n",
    "print(f\"y_test is {y_test}\")\n",
    "print(f\"y_train is {y_train}\")   # to \"get a visual\" on these...\n",
    "# print(X_test)\n",
    "# print(X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING the input scaler:\n",
      "\n",
      "X_train_scaled (first 5 rows): [[ 0.97986746  0.47658613  0.15564255 -0.11001639]\n",
      " [ 0.51168555  0.76581402  0.82692531  1.13316885]\n",
      " [-0.54172377  0.01382151  0.021386   -0.11001639]\n",
      " [-0.54172377 -1.54800909 -1.45543607 -1.35320164]\n",
      " [-1.00990568 -1.20093562 -1.18692297 -1.35320164]]\n"
     ]
    }
   ],
   "source": [
    "# for most NNets, it's important to keep the feature values in the -1-to-1 range\n",
    "#    This is done through the \"StandardScaler\" in scikit-learn\n",
    "# \n",
    "USE_SCALER = True   # this variable is important! It tracks if we need to use the scaler...\n",
    "\n",
    "# we \"train the scaler\"  (computes the mean and standard deviation)\n",
    "if USE_SCALER == True:\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    # Scale with the training data! ave becomes 0; stdev becomes 1\n",
    "    scaler.fit(X_train)   \n",
    "    \n",
    "if USE_SCALER == True:    # if we're using the scaler, all inputs need to be scaled...\n",
    "    X_train_scaled = scaler.transform(X_train) # scale!\n",
    "    X_test_scaled = scaler.transform(X_test) # scale!\n",
    "    print(\"USING the input scaler:\\n\")\n",
    "else:\n",
    "    X_train_scaled = X_train.copy()  # not using the scaler\n",
    "    X_test_scaled = X_test.copy()  # not using the scaler\n",
    "    print(\"NOT using the input scaler:\\n\")\n",
    "    \n",
    "y_train_scaled = y_train  # the predicted/desired labels are not scaled\n",
    "y_test_scaled = y_test  # not using the scaler\n",
    "    \n",
    "# ascii_table(X_train_scaled,y_train_scaled)  # unweildy\n",
    "print(f\"X_train_scaled (first 5 rows): {X_train_scaled[:5,:]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "++++++++++  TRAINING:  begin  +++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "++++++++++  TRAINING:   end  +++++++++++++++\n",
      "\n",
      "\n",
      "The (sq) prediction error (the loss) is 0.029531407080073156\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "#\n",
    "# Here's where you can change the number of hidden layers\n",
    "# and number of neurons!\n",
    "#\n",
    "nn_regressor = MLPRegressor(hidden_layer_sizes=(10,10), max_iter=400, activation=\"tanh\",\n",
    "                    solver='sgd', verbose=False, shuffle=True,\n",
    "                    random_state=None, # reproduceability!\n",
    "                    learning_rate_init=.1, learning_rate = 'adaptive')\n",
    "\n",
    "print(\"\\n\\n++++++++++  TRAINING:  begin  +++++++++++++++\\n\\n\")\n",
    "nn_regressor.fit(X_train_scaled, y_train_scaled)\n",
    "print(\"\\n\\n++++++++++  TRAINING:   end  +++++++++++++++\\n\\n\")\n",
    "\n",
    "print(f\"The (sq) prediction error (the loss) is {nn_regressor.loss_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ascii_table_for_regressor(X,y,nn):\n",
    "    \"\"\" a table including predictions using nn.predict \"\"\"\n",
    "    predictions = nn.predict(X) # all predictions\n",
    "    # measure error\n",
    "    error = 0.0\n",
    "    # printing\n",
    "    print(f\"{'input ':>18s} ->  {'pred':^6s}  {'desr':^6s}  {'absdiff':^10s}\") \n",
    "    for i in range(len(y)):\n",
    "        pred = predictions[i]\n",
    "        desired = y[i]\n",
    "        result = abs(desired - pred)\n",
    "        error += result\n",
    "        # of use X_train to see the unscaled...\n",
    "        print(f\"{X_train[i,:]!s:>18s} ->  {pred:<+6.3f}  {desired:<+6.3f}  {result:^10.3f}\") \n",
    "    print(f\"\\naverage abs error: {error/len(y)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            input  ->   pred    desr    absdiff  \n",
      " [6.8 4.8 1.4 1. ] ->  +3.479  +3.800    0.321   \n",
      " [6.4 5.3 1.9 2. ] ->  +3.121  +3.200    0.079   \n",
      " [5.5 4.  1.3 1. ] ->  +3.199  +3.400    0.201   \n",
      " [5.5 1.3 0.2 0. ] ->  +3.048  +3.600    0.552   \n",
      " [5.1 1.9 0.4 0. ] ->  +3.106  +2.800    0.306   \n",
      " [5.1 1.5 0.2 0. ] ->  +2.657  +2.700    0.043   \n",
      " [4.9 1.5 0.1 0. ] ->  +3.338  +3.400    0.062   \n",
      " [6.2 5.4 2.3 2. ] ->  +2.571  +2.200    0.371   \n",
      " [4.4 1.3 0.2 0. ] ->  +3.064  +3.200    0.136   \n",
      " [4.6 1.4 0.3 0. ] ->  +3.262  +3.000    0.262   \n",
      " [6.7 4.7 1.5 1. ] ->  +3.090  +3.000    0.090   \n",
      " [5.  1.4 0.2 0. ] ->  +3.417  +3.800    0.383   \n",
      " [6.  4.5 1.6 1. ] ->  +2.916  +2.700    0.216   \n",
      " [6.3 5.6 1.8 2. ] ->  +4.057  +4.000    0.057   \n",
      " [5.7 4.2 1.2 1. ] ->  +3.234  +3.000    0.234   \n",
      " [6.8 5.9 2.3 2. ] ->  +2.367  +2.000    0.367   \n",
      " [4.9 1.5 0.1 0. ] ->  +3.545  +3.500    0.045   \n",
      " [6.4 4.5 1.5 1. ] ->  +2.983  +3.300    0.317   \n",
      " [5.6 4.5 1.5 1. ] ->  +2.662  +3.000    0.338   \n",
      " [6.9 5.7 2.3 2. ] ->  +3.416  +3.500    0.084   \n",
      "     [6. 4. 1. 1.] ->  +3.662  +3.400    0.262   \n",
      " [5.8 4.1 1.  1. ] ->  +2.552  +2.600    0.048   \n",
      " [5.8 4.  1.2 1. ] ->  +3.159  +3.200    0.041   \n",
      " [6.7 5.7 2.5 2. ] ->  +2.891  +2.900    0.009   \n",
      " [6.5 5.2 2.  2. ] ->  +2.813  +2.700    0.113   \n",
      " [5.2 1.4 0.2 0. ] ->  +2.451  +2.500    0.049   \n",
      " [5.7 5.  2.  2. ] ->  +3.583  +3.700    0.117   \n",
      " [7.6 6.6 2.1 2. ] ->  +3.670  +3.700    0.030   \n",
      "\n",
      "average abs error: 0.18331504714465288\n",
      "\n",
      "\n",
      "+++++ parameters, weights, etc. +++++\n",
      "\n",
      "\n",
      "weights/coefficients:\n",
      "\n",
      "[[ 0.45605887  0.30489175 -0.22022842 -0.01391684 -0.42659564 -0.67093551\n",
      "  -0.0084575   0.39991104 -0.65040911  0.24330982]\n",
      " [-0.59152537  0.18312598  0.15375218 -0.05153944  0.30840122  0.08679739\n",
      "  -0.52960049  0.10876787 -0.69093977 -0.68274288]\n",
      " [ 0.35728374 -0.74672957  0.55907285 -0.61983403  0.34507918 -0.47457355\n",
      "   0.59351995  0.80256686 -0.2555963  -0.25101249]\n",
      " [-0.17346413 -0.90151019 -0.70025641 -0.41608823 -0.67642535  0.32525547\n",
      "  -0.03929637 -0.1898071  -0.23666261  0.43343767]]\n",
      "[[-0.01368826 -1.09770166  0.56871505  0.31257382 -0.08160503 -0.45490953\n",
      "   0.28635337 -0.83088375  0.71541112 -0.81346619]\n",
      " [-0.39371931 -0.72627039  0.50141663  0.48278369  0.29444104 -0.36888435\n",
      "   0.25809667  0.11861739  0.93984282 -0.16686267]\n",
      " [-0.24354199 -0.02616213  0.09503335 -0.51478616 -0.1878701   0.23628088\n",
      "  -0.25151861  0.14166574  0.04250355 -0.31080084]\n",
      " [-0.10999157 -0.31718784 -0.24453247 -0.42771595  0.34406642  0.14876341\n",
      "  -0.19400615  0.38289579  0.42738326 -0.36713906]\n",
      " [-0.41328385 -0.65644658  0.55883243  0.08951154 -0.11461222 -0.25803687\n",
      "   0.32854614 -0.26022449  0.0675365  -0.71793979]\n",
      " [ 0.04010031 -0.35673872 -0.3485528   0.65354886 -0.23372589 -0.13124404\n",
      "   0.30980739 -1.03048825 -0.41314277 -0.74522472]\n",
      " [ 0.04462665  0.0445881   0.41125048 -0.47465248  0.23989796  0.64823752\n",
      "  -0.52366062  0.41377386 -0.34726729  0.07147074]\n",
      " [ 0.10009906 -0.51396977  0.29061958  0.24143871 -0.35107298 -0.22347844\n",
      "  -0.65809148 -1.29875719 -0.03996844 -0.88078803]\n",
      " [ 0.52701098  0.22494486 -0.11030561  0.10887453  0.0130745   0.35577349\n",
      "  -0.03337754 -0.69112331  0.82004586 -0.07694808]\n",
      " [ 0.49562264  0.70580087  0.06865226 -0.83900906  0.06773772  0.05756458\n",
      "   0.20054402  0.65430473 -0.40170005  0.73602171]]\n",
      "[[ 0.17467615]\n",
      " [-0.02364271]\n",
      " [ 0.51892386]\n",
      " [-1.1761361 ]\n",
      " [-0.22463232]\n",
      " [-0.14105326]\n",
      " [-0.79955456]\n",
      " [-0.27446425]\n",
      " [ 1.60406038]\n",
      " [ 1.18899615]]\n",
      "\n",
      "intercepts: [array([-1.71828903, -0.41895016, -0.07223242,  0.85088023, -0.63179105,\n",
      "       -0.47041608,  0.68967358, -0.44455691, -0.06648596,  0.88636527]), array([ 0.63299541,  1.55885645,  0.18340253, -1.66949076,  0.2506279 ,\n",
      "        0.07866554, -0.24652674,  1.10420317, -0.88674977,  0.83591895]), array([2.05641788])]\n",
      "\n",
      "all parameters: {'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (10, 10), 'learning_rate': 'adaptive', 'learning_rate_init': 0.1, 'max_fun': 15000, 'max_iter': 400, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': None, 'shuffle': True, 'solver': 'sgd', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "# let's see how it did on the test data (also the training data!)\n",
    "#\n",
    "ascii_table_for_regressor(X_test_scaled,\n",
    "                          y_test_scaled,\n",
    "                          nn_regressor)   # this is our own f'n, above\n",
    "#\n",
    "# other things...\n",
    "#\n",
    "nn = nn_regressor  # less to type?\n",
    "print(\"\\n\\n+++++ parameters, weights, etc. +++++\\n\")\n",
    "print(f\"\\nweights/coefficients:\\n\")\n",
    "for wts in nn.coefs_:\n",
    "    print(wts)\n",
    "print(f\"\\nintercepts: {nn.intercepts_}\")\n",
    "print(f\"\\nall parameters: {nn.get_params()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepallen</th>\n",
       "      <th>sepalwid</th>\n",
       "      <th>petallen</th>\n",
       "      <th>petalwid</th>\n",
       "      <th>irisname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>7.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepallen  sepalwid  petallen  petalwid  irisname\n",
       "0         4.6       3.6       1.0       0.2         0\n",
       "1         4.3       3.0       1.1       0.1         0\n",
       "2         5.0       3.2       1.2       0.2         0\n",
       "3         5.8       4.0       1.2       0.2         0\n",
       "4         4.4       3.0       1.3       0.2         0\n",
       "..        ...       ...       ...       ...       ...\n",
       "136       7.9       3.8       6.4       2.0         2\n",
       "137       7.6       3.0       6.6       2.1         2\n",
       "138       7.7       3.8       6.7       2.2         2\n",
       "139       7.7       2.8       6.7       2.0         2\n",
       "140       7.7       2.6       6.9       2.3         2\n",
       "\n",
       "[141 rows x 5 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predicit Feature___petallen\n",
    "\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.6 3.6 1.  0.2 0. ]\n",
      " [4.3 3.  1.1 0.1 0. ]\n",
      " [5.  3.2 1.2 0.2 0. ]\n",
      " [5.8 4.  1.2 0.2 0. ]\n",
      " [4.4 3.  1.3 0.2 0. ]\n",
      " [4.4 3.2 1.3 0.2 0. ]\n",
      " [4.5 2.3 1.3 0.3 0. ]\n",
      " [4.7 3.2 1.3 0.2 0. ]\n",
      " [5.  3.5 1.3 0.3 0. ]\n",
      " [5.4 3.9 1.3 0.4 0. ]\n",
      " [5.5 3.5 1.3 0.2 0. ]\n",
      " [4.4 2.9 1.4 0.2 0. ]\n",
      " [4.6 3.4 1.4 0.3 0. ]\n",
      " [4.6 3.2 1.4 0.2 0. ]\n",
      " [4.8 3.  1.4 0.1 0. ]\n",
      " [4.8 3.  1.4 0.3 0. ]\n",
      " [4.9 3.  1.4 0.2 0. ]\n",
      " [5.  3.6 1.4 0.2 0. ]\n",
      " [5.  3.3 1.4 0.2 0. ]\n",
      " [5.1 3.5 1.4 0.2 0. ]\n",
      " [5.1 3.5 1.4 0.3 0. ]\n",
      " [5.2 3.4 1.4 0.2 0. ]\n",
      " [5.5 4.2 1.4 0.2 0. ]\n",
      " [4.6 3.1 1.5 0.2 0. ]\n",
      " [4.9 3.1 1.5 0.1 0. ]\n",
      " [4.9 3.1 1.5 0.1 0. ]\n",
      " [4.9 3.1 1.5 0.1 0. ]\n",
      " [5.  3.4 1.5 0.2 0. ]\n",
      " [5.1 3.8 1.5 0.3 0. ]\n",
      " [5.1 3.7 1.5 0.4 0. ]\n",
      " [5.1 3.4 1.5 0.2 0. ]\n",
      " [5.2 3.5 1.5 0.2 0. ]\n",
      " [5.3 3.7 1.5 0.2 0. ]\n",
      " [5.4 3.7 1.5 0.2 0. ]\n",
      " [5.7 4.4 1.5 0.4 0. ]\n",
      " [4.7 3.2 1.6 0.2 0. ]\n",
      " [4.8 3.4 1.6 0.2 0. ]\n",
      " [5.  3.  1.6 0.2 0. ]\n",
      " [5.  3.4 1.6 0.4 0. ]\n",
      " [5.  3.5 1.6 0.6 0. ]\n",
      " [5.1 3.8 1.6 0.2 0. ]\n",
      " [5.1 3.3 1.7 0.5 0. ]\n",
      " [5.4 3.9 1.7 0.4 0. ]\n",
      " [5.4 3.4 1.7 0.2 0. ]\n",
      " [5.7 3.8 1.7 0.3 0. ]\n",
      " [4.8 3.4 1.9 0.2 0. ]\n",
      " [5.1 3.8 1.9 0.4 0. ]\n",
      " [7.  3.2 4.7 1.4 1. ]\n",
      " [4.9 2.4 3.3 1.  1. ]\n",
      " [5.  2.3 3.3 1.  1. ]\n",
      " [5.  2.  3.5 1.  1. ]\n",
      " [5.7 2.6 3.5 1.  1. ]\n",
      " [5.6 2.9 3.6 1.3 1. ]\n",
      " [5.5 2.4 3.7 1.  1. ]\n",
      " [5.5 2.4 3.8 1.1 1. ]\n",
      " [5.2 2.7 3.9 1.4 1. ]\n",
      " [5.6 2.5 3.9 1.1 1. ]\n",
      " [5.8 2.7 3.9 1.2 1. ]\n",
      " [5.5 2.3 4.  1.3 1. ]\n",
      " [5.5 2.5 4.  1.3 1. ]\n",
      " [5.8 2.6 4.  1.2 1. ]\n",
      " [6.  2.2 4.  1.  1. ]\n",
      " [6.1 2.8 4.  1.3 1. ]\n",
      " [5.6 3.  4.1 1.3 1. ]\n",
      " [5.8 2.7 4.1 1.  1. ]\n",
      " [5.6 2.7 4.2 1.3 1. ]\n",
      " [5.7 3.  4.2 1.2 1. ]\n",
      " [5.9 3.  4.2 1.5 1. ]\n",
      " [6.4 2.9 4.3 1.3 1. ]\n",
      " [5.5 2.6 4.4 1.2 1. ]\n",
      " [6.3 2.3 4.4 1.3 1. ]\n",
      " [6.6 3.  4.4 1.4 1. ]\n",
      " [6.7 3.1 4.4 1.4 1. ]\n",
      " [5.4 3.  4.5 1.5 1. ]\n",
      " [5.6 3.  4.5 1.5 1. ]\n",
      " [5.7 2.8 4.5 1.3 1. ]\n",
      " [6.  2.9 4.5 1.5 1. ]\n",
      " [6.  3.4 4.5 1.6 1. ]\n",
      " [6.2 2.2 4.5 1.5 1. ]\n",
      " [6.4 3.2 4.5 1.5 1. ]\n",
      " [6.1 3.  4.6 1.4 1. ]\n",
      " [6.5 2.8 4.6 1.5 1. ]\n",
      " [6.6 2.9 4.6 1.3 1. ]\n",
      " [6.1 2.9 4.7 1.4 1. ]\n",
      " [6.1 2.8 4.7 1.2 1. ]\n",
      " [6.3 3.3 4.7 1.6 1. ]\n",
      " [6.7 3.1 4.7 1.5 1. ]\n",
      " [5.9 3.2 4.8 1.8 1. ]\n",
      " [6.8 2.8 4.8 1.4 1. ]\n",
      " [6.3 2.5 4.9 1.5 1. ]\n",
      " [6.9 3.1 4.9 1.5 1. ]\n",
      " [6.7 3.  5.  1.7 1. ]\n",
      " [6.  2.7 5.1 1.6 1. ]\n",
      " [4.9 2.5 4.5 1.7 2. ]\n",
      " [6.  3.  4.8 1.8 2. ]\n",
      " [6.2 2.8 4.8 1.8 2. ]\n",
      " [5.6 2.8 4.9 2.  2. ]\n",
      " [6.1 3.  4.9 1.8 2. ]\n",
      " [6.3 2.7 4.9 1.8 2. ]\n",
      " [5.7 2.5 5.  2.  2. ]\n",
      " [6.  2.2 5.  1.5 2. ]\n",
      " [6.3 2.5 5.  1.9 2. ]\n",
      " [5.8 2.8 5.1 2.4 2. ]\n",
      " [5.8 2.7 5.1 1.9 2. ]\n",
      " [5.9 3.  5.1 1.8 2. ]\n",
      " [6.3 2.8 5.1 1.5 2. ]\n",
      " [6.5 3.2 5.1 2.  2. ]\n",
      " [6.9 3.1 5.1 2.3 2. ]\n",
      " [6.5 3.  5.2 2.  2. ]\n",
      " [6.7 3.  5.2 2.3 2. ]\n",
      " [6.4 2.7 5.3 1.9 2. ]\n",
      " [6.4 3.2 5.3 2.3 2. ]\n",
      " [6.2 3.4 5.4 2.3 2. ]\n",
      " [6.9 3.1 5.4 2.1 2. ]\n",
      " [6.4 3.1 5.5 1.8 2. ]\n",
      " [6.5 3.  5.5 1.8 2. ]\n",
      " [6.8 3.  5.5 2.1 2. ]\n",
      " [6.1 2.6 5.6 1.4 2. ]\n",
      " [6.3 2.9 5.6 1.8 2. ]\n",
      " [6.3 3.4 5.6 2.4 2. ]\n",
      " [6.4 2.8 5.6 2.1 2. ]\n",
      " [6.4 2.8 5.6 2.2 2. ]\n",
      " [6.7 3.1 5.6 2.4 2. ]\n",
      " [6.7 3.3 5.7 2.1 2. ]\n",
      " [6.7 3.3 5.7 2.5 2. ]\n",
      " [6.9 3.2 5.7 2.3 2. ]\n",
      " [6.5 3.  5.8 2.2 2. ]\n",
      " [6.7 2.5 5.8 1.8 2. ]\n",
      " [7.2 3.  5.8 1.6 2. ]\n",
      " [6.8 3.2 5.9 2.3 2. ]\n",
      " [7.1 3.  5.9 2.1 2. ]\n",
      " [7.2 3.2 6.  1.8 2. ]\n",
      " [7.2 3.6 6.1 2.5 2. ]\n",
      " [7.4 2.8 6.1 1.9 2. ]\n",
      " [7.7 3.  6.1 2.3 2. ]\n",
      " [7.3 2.9 6.3 1.8 2. ]\n",
      " [7.9 3.8 6.4 2.  2. ]\n",
      " [7.6 3.  6.6 2.1 2. ]\n",
      " [7.7 3.8 6.7 2.2 2. ]\n",
      " [7.7 2.8 6.7 2.  2. ]\n",
      " [7.7 2.6 6.9 2.3 2. ]]\n",
      "\n",
      "The dataset has 141 rows and 5 cols\n"
     ]
    }
   ],
   "source": [
    "A = df_final.values    \n",
    "# print(A)\n",
    "\n",
    "A = A.astype('float64') \n",
    "print(A)\n",
    "\n",
    "NUM_ROWS, NUM_COLS = A.shape\n",
    "print(f\"\\nThe dataset has {NUM_ROWS} rows and {NUM_COLS} cols\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Start of regression experiments... +++\n",
      "\n",
      "X_all (just features: 10 rows) is \n",
      " [[4.6 3.6 0.2 0. ]\n",
      " [4.3 3.  0.1 0. ]\n",
      " [5.  3.2 0.2 0. ]\n",
      " [5.8 4.  0.2 0. ]\n",
      " [4.4 3.  0.2 0. ]\n",
      " [4.4 3.2 0.2 0. ]\n",
      " [4.5 2.3 0.3 0. ]\n",
      " [4.7 3.2 0.2 0. ]\n",
      " [5.  3.5 0.3 0. ]\n",
      " [5.4 3.9 0.4 0. ]]\n",
      "y_all (just petallen)   is \n",
      " [1.  1.1 1.2 1.2 1.3 1.3 1.3 1.3 1.3 1.3 1.3 1.4 1.4 1.4 1.4 1.4 1.4 1.4\n",
      " 1.4 1.4 1.4 1.4 1.4 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.5 1.6\n",
      " 1.6 1.6 1.6 1.6 1.6 1.7 1.7 1.7 1.7 1.9 1.9 4.7 3.3 3.3 3.5 3.5 3.6 3.7\n",
      " 3.8 3.9 3.9 3.9 4.  4.  4.  4.  4.  4.1 4.1 4.2 4.2 4.2 4.3 4.4 4.4 4.4\n",
      " 4.4 4.5 4.5 4.5 4.5 4.5 4.5 4.5 4.6 4.6 4.6 4.7 4.7 4.7 4.7 4.8 4.8 4.9\n",
      " 4.9 5.  5.1 4.5 4.8 4.8 4.9 4.9 4.9 5.  5.  5.  5.1 5.1 5.1 5.1 5.1 5.1\n",
      " 5.2 5.2 5.3 5.3 5.4 5.4 5.5 5.5 5.5 5.6 5.6 5.6 5.6 5.6 5.6 5.7 5.7 5.7\n",
      " 5.8 5.8 5.8 5.9 5.9 6.  6.1 6.1 6.1 6.3 6.4 6.6 6.7 6.7 6.9]\n"
     ]
    }
   ],
   "source": [
    "print(\"+++ Start of regression experiments... +++\\n\")\n",
    "\n",
    "X_all = np.concatenate( (A[:,0:2], A[:,3:]),axis=1)  # columns 0, 1, 3, and 4\n",
    "y_all = A[:,2]             # y (labels) ... is all rows, column 2 (petalwid) only\n",
    "\n",
    "print(f\"X_all (just features: 10 rows) is \\n {X_all[:10,:]}\")\n",
    "print(f\"y_all (just petallen)   is \\n {y_all}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features\n",
      " [[6.4 2.8 2.1 2. ]\n",
      " [5.  3.6 0.2 0. ]\n",
      " [6.7 3.  2.3 2. ]\n",
      " [6.8 3.2 2.3 2. ]\n",
      " [4.9 2.5 1.7 2. ]\n",
      " [5.  3.5 0.3 0. ]\n",
      " [4.6 3.1 0.2 0. ]\n",
      " [6.1 3.  1.8 2. ]\n",
      " [6.  3.  1.8 2. ]\n",
      " [5.6 3.  1.3 1. ]]\n",
      "labels (petallen)\n",
      " [5.6 1.4 5.2 5.9 4.5 1.3 1.5 4.9 4.8 4.1 4.7 6.  1.5 1.5 5.8 1.5 1.  1.5\n",
      " 1.4 4.5 1.5 5.  4.2 5.  6.4 3.8 5.8 5.5 3.9 4.7 5.2 5.4 1.1 4.9 3.9 5.1\n",
      " 1.5 4.3 1.3 1.7 5.1 4.6 1.2 5.6 5.6 5.1 3.3 4.5 4.2 5.1 4.9 5.6 5.7 4.6\n",
      " 3.9 5.6 1.4 4.6 1.6 4.  6.9 1.3 1.3 1.7 6.7 4.5 5.5 1.3 1.4 4.1 1.9 1.4\n",
      " 6.6 3.5 4.  4.7 5.5 6.3 4.5 5.7 4.5 5.8 4.8 5.1 1.6 4.  1.7 1.3 1.5 6.1\n",
      " 3.6 1.4 5.6 1.6 6.1 6.1 5.  5.  1.9 1.6 4.8 4.  6.7 3.3 4.7 3.7 5.1 4.5\n",
      " 5.1 5.9 1.3 4.9 1.6 1.5 5.3 1.4 4.  1.5 1.4 1.4 1.2 1.4 4.8 4.4 1.6 4.5\n",
      " 4.9 4.2 1.5 1.4 5.3 5.7 1.7 4.4 4.4 4.4 1.4 5.4 3.5 4.7 1.5]\n"
     ]
    }
   ],
   "source": [
    "indices = np.random.permutation(len(y_all))  # indices is a permutation-list\n",
    "\n",
    "# we scramble both X and y, necessarily with the same permutation\n",
    "X_all = X_all[indices]              # we apply the _same_ permutation to each!\n",
    "y_all = y_all[indices]              # again...\n",
    "print(\"features\\n\", X_all[:10,:])\n",
    "print(\"labels (petallen)\\n\",y_all)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with 113 rows;  testing with 28 rows\n",
      "y_test is [5.6 1.4 5.2 5.9 4.5 1.3 1.5 4.9 4.8 4.1 4.7 6.  1.5 1.5 5.8 1.5 1.  1.5\n",
      " 1.4 4.5 1.5 5.  4.2 5.  6.4 3.8 5.8 5.5]\n",
      "y_train is [3.9 4.7 5.2 5.4 1.1 4.9 3.9 5.1 1.5 4.3 1.3 1.7 5.1 4.6 1.2 5.6 5.6 5.1\n",
      " 3.3 4.5 4.2 5.1 4.9 5.6 5.7 4.6 3.9 5.6 1.4 4.6 1.6 4.  6.9 1.3 1.3 1.7\n",
      " 6.7 4.5 5.5 1.3 1.4 4.1 1.9 1.4 6.6 3.5 4.  4.7 5.5 6.3 4.5 5.7 4.5 5.8\n",
      " 4.8 5.1 1.6 4.  1.7 1.3 1.5 6.1 3.6 1.4 5.6 1.6 6.1 6.1 5.  5.  1.9 1.6\n",
      " 4.8 4.  6.7 3.3 4.7 3.7 5.1 4.5 5.1 5.9 1.3 4.9 1.6 1.5 5.3 1.4 4.  1.5\n",
      " 1.4 1.4 1.2 1.4 4.8 4.4 1.6 4.5 4.9 4.2 1.5 1.4 5.3 5.7 1.7 4.4 4.4 4.4\n",
      " 1.4 5.4 3.5 4.7 1.5]\n"
     ]
    }
   ],
   "source": [
    "# a common convention:  train on 80%, test on 20%    Let's define the TEST_PERCENT\n",
    "#\n",
    "NUM_ROWS = X_all.shape[0]     # the number of rows\n",
    "TEST_PERCENT = 0.20\n",
    "TEST_SIZE = int(TEST_PERCENT*NUM_ROWS)   # no harm in rounding down\n",
    "\n",
    "X_test = X_all[:TEST_SIZE]    # first section are for testing\n",
    "y_test = y_all[:TEST_SIZE]\n",
    "\n",
    "X_train = X_all[TEST_SIZE:]   # all the rest are for training\n",
    "y_train = y_all[TEST_SIZE:]\n",
    "\n",
    "print(f\"training with {len(y_train)} rows;  testing with {len(y_test)} rows\" )\n",
    "\n",
    "print(f\"y_test is {y_test}\")\n",
    "print(f\"y_train is {y_train}\")   # to \"get a visual\" on these...\n",
    "# print(X_test)\n",
    "# print(X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING the input scaler:\n",
      "\n",
      "X_train_scaled (first 5 rows): [[-0.07403773 -0.80753214 -0.00933928  0.02217846]\n",
      " [ 1.0016247   0.14300048  0.38641286  0.02217846]\n",
      " [ 0.7625886  -0.09463267  1.04599976  1.27526123]\n",
      " [ 1.24066079  0.14300048  1.17791714  1.27526123]\n",
      " [-1.86680844 -0.09463267 -1.46043047 -1.23090431]]\n"
     ]
    }
   ],
   "source": [
    "# for most NNets, it's important to keep the feature values in the -1-to-1 range\n",
    "#    This is done through the \"StandardScaler\" in scikit-learn\n",
    "# \n",
    "USE_SCALER = True   # this variable is important! It tracks if we need to use the scaler...\n",
    "\n",
    "# we \"train the scaler\"  (computes the mean and standard deviation)\n",
    "if USE_SCALER == True:\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    # Scale with the training data! ave becomes 0; stdev becomes 1\n",
    "    scaler.fit(X_train)   \n",
    "    \n",
    "if USE_SCALER == True:    # if we're using the scaler, all inputs need to be scaled...\n",
    "    X_train_scaled = scaler.transform(X_train) # scale!\n",
    "    X_test_scaled = scaler.transform(X_test) # scale!\n",
    "    print(\"USING the input scaler:\\n\")\n",
    "else:\n",
    "    X_train_scaled = X_train.copy()  # not using the scaler\n",
    "    X_test_scaled = X_test.copy()  # not using the scaler\n",
    "    print(\"NOT using the input scaler:\\n\")\n",
    "    \n",
    "y_train_scaled = y_train  # the predicted/desired labels are not scaled\n",
    "y_test_scaled = y_test  # not using the scaler\n",
    "    \n",
    "# ascii_table(X_train_scaled,y_train_scaled)  # unweildy\n",
    "print(f\"X_train_scaled (first 5 rows): {X_train_scaled[:5,:]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "++++++++++  TRAINING:  begin  +++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "++++++++++  TRAINING:   end  +++++++++++++++\n",
      "\n",
      "\n",
      "The (sq) prediction error (the loss) is 0.04504380539067758\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "#\n",
    "# Here's where you can change the number of hidden layers\n",
    "# and number of neurons!\n",
    "#\n",
    "nn_regressor = MLPRegressor(hidden_layer_sizes=(10,10), max_iter=400, activation=\"tanh\",\n",
    "                    solver='sgd', verbose=False, shuffle=True,\n",
    "                    random_state=None, # reproduceability!\n",
    "                    learning_rate_init=.1, learning_rate = 'adaptive')\n",
    "\n",
    "print(\"\\n\\n++++++++++  TRAINING:  begin  +++++++++++++++\\n\\n\")\n",
    "nn_regressor.fit(X_train_scaled, y_train_scaled)\n",
    "print(\"\\n\\n++++++++++  TRAINING:   end  +++++++++++++++\\n\\n\")\n",
    "\n",
    "print(f\"The (sq) prediction error (the loss) is {nn_regressor.loss_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ascii_table_for_regressor(X,y,nn):\n",
    "    \"\"\" a table including predictions using nn.predict \"\"\"\n",
    "    predictions = nn.predict(X) # all predictions\n",
    "    # measure error\n",
    "    error = 0.0\n",
    "    # printing\n",
    "    print(f\"{'input ':>18s} ->  {'pred':^6s}  {'desr':^6s}  {'absdiff':^10s}\") \n",
    "    for i in range(len(y)):\n",
    "        pred = predictions[i]\n",
    "        desired = y[i]\n",
    "        result = abs(desired - pred)\n",
    "        error += result\n",
    "        # of use X_train to see the unscaled...\n",
    "        print(f\"{X_train[i,:]!s:>18s} ->  {pred:<+6.3f}  {desired:<+6.3f}  {result:^10.3f}\") \n",
    "    print(f\"\\naverage abs error: {error/len(y)}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            input  ->   pred    desr    absdiff  \n",
      " [5.8 2.7 1.2 1. ] ->  +5.628  +5.600    0.028   \n",
      " [6.7 3.1 1.5 1. ] ->  +1.446  +1.400    0.046   \n",
      " [6.5 3.  2.  2. ] ->  +5.874  +5.200    0.674   \n",
      " [6.9 3.1 2.1 2. ] ->  +5.868  +5.900    0.032   \n",
      " [4.3 3.  0.1 0. ] ->  +4.961  +4.500    0.461   \n",
      " [5.6 2.8 2.  2. ] ->  +1.439  +1.300    0.139   \n",
      " [5.6 2.5 1.1 1. ] ->  +1.457  +1.500    0.043   \n",
      " [6.9 3.1 2.3 2. ] ->  +5.316  +4.900    0.416   \n",
      " [5.1 3.7 0.4 0. ] ->  +5.254  +4.800    0.454   \n",
      " [6.4 2.9 1.3 1. ] ->  +3.925  +4.100    0.175   \n",
      " [5.5 3.5 0.2 0. ] ->  +4.554  +4.700    0.146   \n",
      " [5.4 3.9 0.4 0. ] ->  +5.960  +6.000    0.040   \n",
      " [5.8 2.7 1.9 2. ] ->  +1.453  +1.500    0.047   \n",
      " [6.6 2.9 1.3 1. ] ->  +1.471  +1.500    0.029   \n",
      " [5.  3.2 0.2 0. ] ->  +5.708  +5.800    0.092   \n",
      " [6.4 2.8 2.2 2. ] ->  +1.452  +1.500    0.048   \n",
      " [6.1 2.6 1.4 2. ] ->  +1.430  +1.000    0.430   \n",
      " [6.  2.7 1.6 1. ] ->  +1.471  +1.500    0.029   \n",
      " [4.9 2.4 1.  1. ] ->  +1.452  +1.400    0.052   \n",
      " [5.7 2.8 1.3 1. ] ->  +4.626  +4.500    0.126   \n",
      " [5.7 3.  1.2 1. ] ->  +1.371  +1.500    0.129   \n",
      " [6.3 2.8 1.5 2. ] ->  +5.387  +5.000    0.387   \n",
      " [6.9 3.1 1.5 1. ] ->  +4.561  +4.200    0.361   \n",
      " [6.3 3.4 2.4 2. ] ->  +4.921  +5.000    0.079   \n",
      " [6.7 3.3 2.1 2. ] ->  +5.824  +6.400    0.576   \n",
      " [6.1 3.  1.4 1. ] ->  +3.558  +3.800    0.242   \n",
      " [5.2 2.7 1.4 1. ] ->  +5.980  +5.800    0.180   \n",
      " [6.3 2.9 1.8 2. ] ->  +5.629  +5.500    0.129   \n",
      "\n",
      "average abs error: 0.19975337920879205\n",
      "\n",
      "\n",
      "+++++ parameters, weights, etc. +++++\n",
      "\n",
      "\n",
      "weights/coefficients:\n",
      "\n",
      "[[ 0.02085151  0.54336062 -1.14711624  1.26298585  0.35926934  0.12194161\n",
      "  -0.35490574  0.05144645  0.93223329  0.16421963]\n",
      " [-0.51508184 -0.38602464  0.45927158  0.25419145  0.19919932 -0.52482218\n",
      "  -0.05817796 -0.11261905  0.13006015 -0.42207134]\n",
      " [ 0.19775384  1.5462427  -0.57417975  0.98584247  0.51266934 -0.23328572\n",
      "   0.3736206   0.69882047  0.08839778  0.31503536]\n",
      " [ 0.0578516   0.77096699 -0.01313382  1.35192587  0.2523203   0.57135953\n",
      "   0.51702068  0.63416161  0.56847374 -0.23686466]]\n",
      "[[ 0.46823917  0.634012   -0.38996869 -0.03416425 -0.64272241  0.49623604\n",
      "  -0.30187575  0.02689814  0.26753572  0.76318278]\n",
      " [ 0.4349409  -0.92768875  0.43729362  0.70647576 -0.25580307 -0.93933998\n",
      "   0.84412011 -0.68238977  0.8271993   0.0267177 ]\n",
      " [-0.1403989  -0.48280389 -0.14192898  0.27155924  0.52332223 -0.15757402\n",
      "  -0.8478856  -0.09725609  0.42278222 -0.34792037]\n",
      " [-0.09770194 -0.38222052  0.18659494  0.27403311 -0.04645475 -0.56500209\n",
      "   0.73157282  0.29029388  0.73137084  0.09660426]\n",
      " [ 0.44825486 -0.58977817  0.947933    0.46556351 -0.01483545 -0.22378728\n",
      "   0.8804849   0.64365287  0.48126702 -0.49840959]\n",
      " [ 0.18455659 -0.07895361  0.35308686  0.3293367  -0.48255786 -0.59043868\n",
      "   0.17254804  0.45194124  0.08885269 -0.13495975]\n",
      " [ 0.7842337  -0.79672768  0.61734153  0.99510178 -0.09416176 -1.06377483\n",
      "   0.35677171  0.1306487   0.76806103 -0.70340128]\n",
      " [ 0.48400844 -1.00824072  0.44192573  0.18936683  0.05301693 -0.90722838\n",
      "   0.68075458 -0.06640902  0.94077048  0.0141035 ]\n",
      " [ 0.19072507 -0.16896     0.59811594 -0.41084924  0.06006944  0.01658058\n",
      "   0.65184444  1.02917526  0.37760597 -0.12832226]\n",
      " [ 0.23497419  0.38131398 -0.83258895 -0.61421907 -1.2690175   0.4701919\n",
      "  -0.01227883  0.27680173  0.34481506  0.86172133]]\n",
      "[[ 0.80071597]\n",
      " [ 0.00257422]\n",
      " [-0.54025031]\n",
      " [-0.48936924]\n",
      " [ 1.47757271]\n",
      " [-0.73634872]\n",
      " [ 0.9716422 ]\n",
      " [ 0.81798079]\n",
      " [ 0.25192153]\n",
      " [ 0.2017982 ]]\n",
      "\n",
      "intercepts: [array([-1.75903484,  2.42992486, -0.30234815,  0.72649801, -0.34736556,\n",
      "       -0.37670291,  0.25440727, -0.45842327, -0.8696492 , -1.01942665]), array([ 0.62191081, -1.90668344,  0.93837194,  0.76390893,  1.57707869,\n",
      "       -1.5706973 ,  0.10330233, -0.88492876,  0.69478309, -0.99452128]), array([2.93853642])]\n",
      "\n",
      "all parameters: {'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (10, 10), 'learning_rate': 'adaptive', 'learning_rate_init': 0.1, 'max_fun': 15000, 'max_iter': 400, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': None, 'shuffle': True, 'solver': 'sgd', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "# let's see how it did on the test data (also the training data!)\n",
    "#\n",
    "ascii_table_for_regressor(X_test_scaled,\n",
    "                          y_test_scaled,\n",
    "                          nn_regressor)   # this is our own f'n, above\n",
    "#\n",
    "# other things...\n",
    "#\n",
    "nn = nn_regressor  # less to type?\n",
    "print(\"\\n\\n+++++ parameters, weights, etc. +++++\\n\")\n",
    "print(f\"\\nweights/coefficients:\\n\")\n",
    "for wts in nn.coefs_:\n",
    "    print(wts)\n",
    "print(f\"\\nintercepts: {nn.intercepts_}\")\n",
    "print(f\"\\nall parameters: {nn.get_params()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
